# 模型量化

https://zhuanlan.zhihu.com/p/551916748

https://zhuanlan.zhihu.com/p/58182172

https://zhuanlan.zhihu.com/p/364782854

https://zhuanlan.zhihu.com/p/516116539

https://github.com/Ewenwan/MVision/tree/master/CNN/Deep_Compression/quantization

## 1 量化技术的必要性

训练好的模型的[权重](https://mp.weixin.qq.com/s?__biz=Mzg3ODU2MzY5MA==&mid=2247487770&idx=1&sn=749d1719844dcfe5f87dae2de65a8b6b&chksm=cf10891ff86700096be8eeb671004693adca26051c3e75628dedb87a95ed857aadc0502c10b4&token=1276531538&lang=zh_CN#rd)一般来说都是**FP32**也就是单精度浮点型，在深度学习训练和推理的过程中，最常用的精度就是FP32。当然也会有FP64、FP16、BF16、TF32等更多的精度：

![](https://pic2.zhimg.com/v2-5fb5dff51514ed3ec16640d92b4b21b5_r.jpg)

FP32 是单精度浮点数，用8bit 表示指数，23bit 表示小数；FP16半精度浮点数，用5bit 表示指数，10bit 表示小数；BF16是对FP32单精度浮点数截断数据，即用8bit 表示指数，7bit 表示小数。TF32 是一种截短的 Float32 数据格式，将 FP32 中 23 个尾数位截短为 10 bits，而指数位仍为 8 bits，总长度为 19 (=1 + 8 + 10) bits。

对于浮点数来说，指数位表示该精度可达的动态范围，而尾数位表示精度。之前老潘的一篇[文章](https://mp.weixin.qq.com/s?__biz=Mzg3ODU2MzY5MA==&mid=2247487770&idx=1&sn=749d1719844dcfe5f87dae2de65a8b6b&chksm=cf10891ff86700096be8eeb671004693adca26051c3e75628dedb87a95ed857aadc0502c10b4&token=1276531538&lang=zh_CN#rd)中提到，FP16的普遍精度是 `~5.96e−8 (6.10e−5) … 65504`，而我们模型中的FP32权重有部分数值是 `1e-10`级别。这样从FP32->FP16会导致部分精度丢失，从而模型的精度也会下降一些。

![](https://pic3.zhimg.com/v2-1a10fe6cf14bef46eed4e9ec88e0d52a_r.jpg)

其实从FP32->FP16也是一种量化，只不过因为FP32->FP16几乎是无损的(CUDA中使用 `__float2half`直接进行转换)，不需要 `calibrator`去校正、更不需要 `retrain`。

而且FP16的精度下降对于[大部分任务](https://mp.weixin.qq.com/s?__biz=Mzg3ODU2MzY5MA==&mid=2247487034&idx=1&sn=728b01c2b38c436fd337dad0038e1f0a&chksm=cf10963ff8671f2995d2b3a4dc9192f2b2b40dfecadc31d409c88b47c492c866758cb87af65a&token=1276531538&lang=zh_CN#rd)影响不是很大，甚至有些任务会提升。NVIDIA对于FP16有专门的Tensor Cores可以进行矩阵运算，相比FP32来说吞吐量提升一倍。

![](https://pic1.zhimg.com/v2-0893272bd4a45b3a40b845928c4ed4ec_r.jpg)

实际点来说， **量化就是将我们训练好的模型，不论是权重、还是计算op，都转换为低精度去计算** 。因为FP16的量化很简单，所以实际中我们谈论的量化更多的是 **INT8的量化** ，当然也有3-bit、4-bit的量化，不过目前来说比较常见比较实用的，也就是INT8量化了，之后老潘的重点也是INT8量化。

那么经过INT8量化后的模型：

* 模型容量变小了，这个很好理解，FP32的权重变成INT8，大小直接缩了4倍
* 模型运行速度可以提升，实际卷积计算的op是INT8类型，在特定硬件下可以利用INT8的指令集去实现高吞吐，不论是GPU还是INTEL、ARM等平台都有**INT8的指令集优化**
* 对于某些设备，使用INT8的模型耗电量更少，对于嵌入式侧端设备来说提升是巨大的

所以说，随着我们模型越来越大，需求越来越高，模型的量化自然是少不了的一项技术。

如果你担心INT8量化对于**精度**的影响，我们可以看下NVIDIA量化研究的一些结论:

![](https://pic3.zhimg.com/v2-1b5d758d2ef30e90af69e658575c22d6_r.jpg)

出自《INTEGER QUANTIZATION FOR DEEP LEARNING INFERENCE: PRINCIPLES AND EMPIRICAL EVALUATION》，文末有下载链接。

## 2 量化基本知识

量化的 **两个重要过程** ，一个是量化（Quantize），另一个是反量化（Dequantize）：

* 量化就是将浮点型实数量化为整型数（FP32->INT8）
* 反量化就是将整型数转换为浮点型实数（INT8->FP32）

![](https://pic1.zhimg.com/v2-3e76e3a88e26e096a17c0a78d7be56e4_r.jpg)

量化和反量化操作在最终的模型推理中都会用到，接下来我们就具体说下。

之后实数就代表我们的FP32浮点数，而整数就代表INT8整型数。

### **量化操作**

比如有一个FP32的浮点型数字x=5.234，然后我们需要把这个数变为整型，也就是要量化它，怎么搞。我们可以把这个数字乘上一个量化系数s，比如s=100，那么量化后的值xq=x∗s=5.234∗100=523.4，然后我们对这个数字进行四舍五入（也就是round操作）最终为

xq=round(x∗s)=round(5.234∗100)=523

这样就行了吗， **523有点大啊** ，我们的整型INT8的范围是[-128,127]，无符号INT8的范围也才[0-255]，这个量化后的值有点放不下呀。

怎么办，当然是要截断了，假设我们的INT8范围是[−2b−1,2b−1−1]，因为我们使用的是INT8，所以这里的b=8，那么上述的式子又可以变为：

xq=clip(round(x∗s),−2b−1,2b−1−1)=clip(round(5.234∗100),−128,127)=127

这样就结束了么？

当然没有，刚才的这个数字x=5.234，被映射到了127，那么如果是x=0呢？貌似直接带入算出来也是0，但是这样做对么？

### **基于线性量化的对称量化和非对称量化**

对不对的关键在于我们是否是采用 **对称量化** ，什么是对称量化呢？这里的对称指的是以**0为中心**进行量化（还有另一种说法，这里老潘先略过），然后0两边的动态范围都是一样的。

![](https://pic3.zhimg.com/v2-97354bedca06e959cd19bcab197118e2_r.jpg)

可以看上图，左边是非对称量化，右边是对称量化（也称为Affine quantization和Scale quantization）。可以观察到：

* 对称量化的实数0也对应着整数的0，而非对称量化的实数0不一定对应着整数0，而是z。
* 对称量化实数的范围是对称的（[−α,α]），而非对称量化的则不对称([−β,α])
* 对称量化整数的范围是对称的（[-127,127]），而非对称量化的则不对称（[-128,127]）

所以上述的非对称量化过程可以简述为f(x)=s·x+z，其中z是 `zero-point`，这个数字就代表 **实数0映射到整数是多少** ，而对称量化则是f(x)=s·x。

这样就明白了刚才的问题：*如果是*x=0 *呢？貌似直接带入算出来也是0* ，如果我们采用的是 **对称量化** ，那就没问题！

需要说明一点，不论是非对称还是对称量化，是基于线性量化（也可以称作均匀量化）的一种。线性量化将FP32映射到INT8数据类型， **每个间隔是相等的** ，而不相等的就称为非线性量化。非线性量化因为对部署并不是很友好，虽然能够更好地捕捉到权重分布的密集点，但感觉用的并不多，这里也就先不多说了。

关于详细的 `非对称量化，对称量化对比`可以参考这篇文章：

* [Affine Quantization vs Scale Quantization](https://iq.opengenus.org/affine-quantization-vs-scale-quantization/)

### **对称量化**

接下来的重点是 **对称量化** ，也就是TensorRT中使用的量化方式，这里的范围也就是[-127,127]，因为只比[-128,127]少了一个范围，所以实际量化中并没有太大的影响。

话说回来，上文量化操作中，量化系数随便说了个s=100，这个当然是不对的，这个s需要根据我们的**实际数据分布**来计算。

![](https://pic3.zhimg.com/v2-a602c2007176bcab46119ac32656f40e_r.jpg)

如上式，α代表当前输入数据分布中的实数最大值，因为是对称，因此实际范围是[−α,α]。而b=8代表INT8量化，那么上述的量化公式就是之前提到的对称量化公式。

可以对比下非对称和对称的量化公式，对称量化因为z=0，所以公式简化了很多。

![](https://pic3.zhimg.com/v2-18e748f0d875b9d26a1c31f2f314f00a_r.jpg)

对于对称量化，假设当前根据权重分布，选取的α为4，那么s=127/α=127/4=31.75。

如下式子，在反量化的时候我们需要将反向操作一番，将量化后的结果乘以1/s重新变为浮点型。这里其实也就相当于乘以α/127，因为有1/s=1/(127/α)=α/127。

![](https://pic4.zhimg.com/v2-860edd8486e09400e273dc7857046317_r.jpg)

那么实际操作过程中，scale系数是怎么用呢？或者说s这个量化系数是怎么作用于所有的输入、所有的权重呢？

一般量化过程中，有 `pre-tensor`和 `pre-channel`两种方式，`pre-tensor`显而易见，就是对于同一块输入（比如某个卷积前的输入tensor）我们采用一个scale，该层所有的输入数据共享一个scale值；而 `pre-channel`呢一般是作用于权重，比如一个卷积的权重维度是[64,3,3,3]（输入通道为3输出通道为64，卷积核为3x3），`pre-channel`就是会产生64个scale值，分别作用于该卷积权重参数的64个通道。

为什么权重不能是 `pre-tensor`呢？这个对精度的影响太大了，所以一般不用。输入就可以 `pre-tensor`？当然可以，也经过测试了，对精度的影响不是很大，完全可以用。

那为什么权重必须是 `pre-channel`呢？不能是每个权重值都有自己的scale么？呃，这个问题嘛，首先可以想到，这个计算量，应该挺大，其次嘛，让我们分析一下。

### **卷积操作量化**

铺垫了这么多，那么接下来说下**量化最核心**的操作吧，量化过程中最核心的操作当然是 **卷积量化** 。

我们都知道卷积操作可以拆分为[im2col+sgemm](https://mp.weixin.qq.com/s?__biz=Mzg3ODU2MzY5MA==&mid=2247488158&idx=1&sn=3722bc7433811d494e179cb828dade32&chksm=cf108a9bf867038d4e7e451212429925a48dcbd47d9811315c132271f104540fe503555e7611&token=1276531538&lang=zh_CN#rd)，而大部分的计算都在**矩阵运算**也就是**sgemm**中，我们量化的重点也就是这个操作。以前是FP32计算，而现在变成INT8去计算，这是怎么转换的呢？

接下来重点分析一下 **量化公式** ！注意！这个很重要！

首先，矩阵相乘可以表示为Y=XW，X为输入W为权重，Y为输出。偏置bias一般可以去掉，对精度影响也不大，所以就先不考虑了。

![](https://pic3.zhimg.com/v2-bd1b908ac4dd446410db90ea3b966eb2_r.jpg)

注意看上图输入X的维度为[m,p]而W的维度为[p,n]，因此i的范围为[0,m)，k的范围为[0,p)。W和Y同理。这里的输入和权重都是FP32精度，也就是实数。

而对应的INT8精度的输入和权重为，q下标就代表quantize也就是量化：

![](https://pic2.zhimg.com/v2-63ccf97f6ba64f84bd3d8ad03062dba1_r.jpg)

接下来，我们把矩阵公式细粒度拆成一个一个计算，也就是行和列每个元素相乘然后求和：

![](https://pic4.zhimg.com/v2-0bf23ff1a9d642fcb4d9bc2c6acd633f_r.jpg)

首先是最左边，xik和wkj分别代表浮点型的输入和权重，i代表第i行，k代表第k列，因此xik代表第i行，第k列的元素，wki同理。**两者相乘求和**就可以得到yij，可以看到这里求和的范围是p，k从1到p变化。

进一步，两个浮点型的运算可以被近似为**INT8反量化后**的运算，进一步等于量化后的运算:

![](https://pic4.zhimg.com/v2-3dce58d8e3a11d1ba512e75cca79234f_r.jpg)

可以看到上式每个元素都有自己的scale值，也就是s， **而我们也必须把x和w的scale值提取到前面才能让x和w实现INT8类型的矩阵运算** ：

![](https://pic2.zhimg.com/v2-e4f9bcd76fe75fa9c237e2785cd8bca5_r.jpg)

这里可以发现，如果想要把这两个scale元素，也就是sx,i和sw,j提出来，那么这个k必须干掉，这里可以暂停一下想下为什么？

![](https://pic4.zhimg.com/v2-914fc78c8515af0524c99ab9351407a3_r.jpg)

当把k去除将s取出来之后，我们发现sx,i和sw,j分别代表输入的第i行的scale和权重的第j列的scale值，这样输入的每一行必须共享scale，而权重的每一列也必须共享scale！

![](https://pic3.zhimg.com/v2-78260ba7e4c1c66267023d3ad0273012_r.jpg)

那么 `pre-channel`又是怎么来的呢？

还记得之前说过的[im2col+sgemm](https://mp.weixin.qq.com/s?__biz=Mzg3ODU2MzY5MA==&mid=2247488158&idx=1&sn=3722bc7433811d494e179cb828dade32&chksm=cf108a9bf867038d4e7e451212429925a48dcbd47d9811315c132271f104540fe503555e7611&token=1276531538&lang=zh_CN#rd)操作吗（如果不记得强烈建议去看看），其中的 `sgemm`是这样的，需要注意，下图左边的kernel矩阵，每一行代表一个输出通道的kernel集合（这里因为输入图像是三通道的，因此kernel有三个，不同颜色代表一个kernel）：

![](https://pic2.zhimg.com/v2-8be07ed1b10f071e35d6ec9720218249_r.jpg)

这就是 `pre-channel`或者详细点就是 `per-output-channel`也就是卷积输出通道，我们对每一个卷积权重的输出通道那一维进行量化，然后共享一个scale，这也就呼应了上述的公式！
