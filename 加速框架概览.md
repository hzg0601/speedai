# 加速框架概览

[开源AI框架学习路线](https://zhuanlan.zhihu.com/p/467904500)

![](https://pic3.zhimg.com/v2-b6774b41dad71e96639d87dc036d600e_r.jpg)

整体上，左侧采用以生产流程环节的不同方式来呈现。由上至下依次为：数据准备阶段、（分布式）训练阶段、编译优化阶段、推理生产阶段。最下层为硬件基础。而右侧上部分为提高研发效能的辅助系统。下半部分是分布式基础设施。

## 1 推理加速框架(部署工具)

### 1.0 若干基本概念

#### 1.0.1中间表示IR

IR这个词经常在深度学习框架中出现。IR全称Intermediate Representation，中文翻译成中间表示。中间表示四个字拆开来都能看懂，合起来到底是个什么意思呢？实际上很多优秀的文章已经剖析、探讨过这个话题了，但是这里我想用另一种不带技术专业术语的方式来谈谈我对深度学习中的IR理解。也许有些理解也不够到位，甚至会有偏差。

* **定义**

先看一下wiki里的定义：

> *“An Intermediate representation (IR) is the data structure or code used internally by a compiler or virtual machine to represent source code. An IR is designed to be conducive for further processing, such as optimization and translation. A "good" IR must be* *accurate* *– capable of representing the source code without loss of information – and* *independent* *of any particular source or target language. An IR may take one of several forms: an in-memory data structure, or a special tuple- or stack-based code readable by the program. In the latter case it is also called an intermediate language.”*  *- Wikipedia*

在编译器或虚拟机内部来代表源代码的表示，可能是数据结构也可能是代码，因此形式可以是不定的。深度学习框架中的IR概念也是启发自编译器。有些框架用protocol协议来定义，有些就直接使用C++ native的形式定义。在深度学习框架里IR想表示的是计算过程。需要注意的是：

1. IR结构也有多种，在深度学习里是Graph-based IR，这是由于深度学习框架的特定结构导致的。基本上深度学习框架都是由一张DAG计算图来展现，因此基于图的IR基本就能够满足需求了。
2. IR可以有多级，也就是所谓的multi-stage。下文也会提到。一般展现给用户的是Graph级别的IR。

说了半天，对于没有背景知识的人来说还是很抽象，听不懂啊。还是不知道IR具体是什么东西，也不知道到底含括哪些定义范围。那就用通俗点的语言来翻译一下。

* **解读**

IR就是一种语言。语言用于表达想法，IR用来表示执行过程的描述。各个框架想象成是世界上不同的国家。我们都知道每个国家都有自己的官方语言，所以每个框架基本上也都有自己的一套语言，这个语言就是IR。这个时候说的IR是图级别的，算是高级语言。有高级(high-level)就会有低级(low-level)，这个会在后面讲到。这里的高级、低级并非我们生活中所说的意思，不代表低级就是不好的意思。可以将高级理解为通用性强，易被理解。越往下则越抽象，学习成本越高。「高级与低级是相对的，需要结合具体的使用场景来看。」

* **翻译官：转换工具**

大家有了各自语言之后发现由于听不懂互相之间的语言导致不能直接交流沟通。那这个时间就需要有人来翻译一下，这个翻译工作谁来做？就是我们经常说的转换工具。这个工具就专门干这个事情，把别的框架定义的语言翻译到自己定义的语言体系中。翻译过程中可能会遇到其他语言中的有些词语在自己的语言里没有直接对应的词汇，那么就需要进行意译而不是直接翻译。回到转换工具里就是需要做合理化，换个类型来表示不支持的类型得出正确计算结果就可以。

这么看来转换工具作为交流的桥梁是个很重要的工具啊。但是有个问题又出现了，这世界上有很多种语言啊，总不可能一个个都翻译过来吧，工作量剧增。且可能随着新语言的不断涌现，需要不断的新增。这个时候就有人提出我们来搞一个统一通用的语言吧，大家都对接到我这来，我来给你们做统一的翻译，大家都能听懂我的话就行了，比如ONNX（ *Open Neural Network Exchange* ）。这就跟英语是国际上通用语言是一个道理啊。虽然不懂各自的语言，但是我们可以英语交流。「有部分框架是直接使用onnx来作为自己的表示，没有重新去设计一套语言。」所以大部分的框架的转化工具会把支持ONNX作为首选，支持了onnx基本上大部分其他框架只要是onnx支持的都能翻译过来。

* **语言专家：图优化器**

转换工具完成了语言间的转接，而图优化器是针对本土语言的。那它需要优化什么呢？回想一下做总结报告写文档的时候，往往第一版写出来都是不太让人满意的。要么内容冗余，要么有些段落意图不明显没有提炼出重点。作者可能真的是尽力了，即使再修改几个版本整体看来还是不太精简优雅。这个时候我们就可以求助专业人士来润色一下。这个时候图优化器就登场了，对于传进来的计算描述，有些地方该简化的地方简化，冗余部分该删除删除。经过它的一顿操作，你就获得了专家认为目前最好的输出。但是说到底这个图优化器还是基于模版来实现的，可能是个伪专家。优化手段都是有固定模板的，只要匹配上自己定义的pattern就做相对应的修改。这样的做法可能会导致两个情况（这句是废话）：1. 结果挺不错的， 2: 局部看起来不错，通篇整体看乱七八糟。这好比什么呢？做个不是那么恰当的比喻：你把自己写好的文档发给在x宝上找的文章修改店家。实际上也不是真的人工帮你修改，店家直接上传到不知道谁开发的一个模版替换系统点了下上传、润色、保存三个按钮。然后也不管保存出来的是个什么就直接给你发货了。最终结果就跟开盲盒一样，不知道里面是惊喜还是惊吓。

* **语言修订：IR version**

语言是用来使用的，在使用过程中伴随着发展，相对应的就需要调整。举个例子，古诗中的**一骑红尘妃子笑，** 这个**骑**到底念什么音？至少在我上学的时候学的是读 **jì** ，但后来被改成**qí**了。「这里我们不去探究是出于什么考虑。」那改了之后是不是要昭告所有人，我这边改了啊，以后要这么用。作为权威的汉语字典了就要做出修改，并在出版的时候会标注这是第几个修订版本。那么对于深度学习里面的语言来说也是一样的。也许有些语义不再适用，那么就需要去除。改完之后就得告诉使用者，这个版本做了哪些改动，最后支持了什么，对外展示就是Version x.x。同时新版本需要有兼容性。什么是兼容？还是开头说的例子，那我现在就读**一骑（jì）红尘妃子笑**行不行？一般不会认为你是错的（避开应试教育考试场景）。那这就是兼容了。兼容是出于目前主流广大使用者考虑，很多人一时接受、转换不过来。但新一代被教育者接受的信号都是后者，慢慢地就没有人会去使用旧时的读法，自然而然也就会被摒弃了。

* **多级IR**

多级IR的主要目的是把问题拆成多步来解决，那么一个复杂的问题就被拆解成了多个子问题来解决。「当IR从Graph IR逐级lower+优化到后端IR时，就可以认为这是个编译器链路了。所以TVM、MLIR等框架管自己叫深度学习编译器。」multi-stage的设计旨在每个子问题找到最优解。这样的解题思路存在的问题是所有局部的最优解加起来就是全局最优解么？答案是否定的。前一段时间TVM发布的在深度学习编译器新方向探索的文章中就提到了这个问题。

因为经过层层翻译发生了不可逆的部分信息损失，导致层级间存在边界问题。下级只能单向接受上游传来的结果，丢失了全局信息。因此需要层级间的交互交流来保障全局信息。什么意思呢？再举个不是特别合适的例子：

**现在有一段法语文字，请翻译成中文。** 最直接的方式肯定就是直接从法语翻译到中文，没有中间人传话，尽可能保障不失真。现在用翻译工具并再加一道工序来进行翻译，法语-->英语-->中文（就是玩儿？）。在法语到英语的过程就可能丢失一些语义了，再从英语到汉语得出的结果可能就不是原来想表达的意思了。这个例子不恰当的地方在于这里并没有high-level，low-level的概念，而相同点在于这个翻译器单次只支持2种语言间的转换。也就是从英语到汉语的时候已经不知道原来的法语原文是什么了。

* **IR的重要性**

IR是深度学习框架定制的交流规则，大家都通过这个规则来沟通。如果没有这样的规则就无法进行交流。且回顾一下大部分组件，都是在对IR进行操作。IR的地位不言而喻。当某个框架被广泛使用时，也就意味着其所定制的IR被广大群众使用，那么对于它的任意改动都可能造成不小的影响。

当需要新创建一门语言的时候就需要慎重地思考以下几个问题：

1. 什么是好的IR？
   能清楚、简洁地呈现自己所想表达的，达成既有目标就是好的IR。
2. 如何去设计IR？
   1. 把IR看成语言后，设计IR的问题就已经转换成如何设计一门语言。想要设计好一门语言不是一件容易的事情。
   2. 思考为什么要重新设计一套IR，是为了解决什么问题。
   3. 考虑清楚后明确目标，是为了能被更多的人理解还是想要更细节精准的描述。
      通常想表达的东西越多IR就会越复杂（这也是废话文学）。
   4. 了解目前主流语言体系，形成一个大局观。「不太会脱离现有的体系。」
   5. 参考主流语言的设计，并需要思考他们为什么要这么设计，能带给自己的启发是什么。

### 1.1 加速推理器TensorRT(NVIDIA)

TensorRT是[nvidia](https://so.csdn.net/so/search?q=nvidia&spm=1001.2101.3001.7020)家的一款高性能深度学习推理SDK。此SDK包含深度学习推理优化器和运行环境，可为深度学习推理应用提供低延迟和高吞吐量。在推理过程中，基于TensorRT的应用程序比仅仅使用CPU作为平台的应用程序要快40倍。

TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎所有的深度学习框架，将TensorRT和NVIDIA的GPU结合起来，能在几乎所有的框架中进行快速和高效的部署推理。

TensorRT 是一个C++库，从 TensorRT 3 开始提供C++ API和Python API，主要用来针对 NVIDIA GPU进行高性能推理（Inference）加速，它可为深度学习推理应用提供低延迟和高吞吐量。

一般的深度学习项目，训练时为了加快速度，会使用多 GPU 分布式训练。但在部署推理时，为了降低成本，往往使用单个 GPU 机器甚至嵌入式平台（比如 NVIDIA Jetson）进行部署，部署端也要有与训练时相同的深度学习环境，如 caffe，TensorFlow 等。由于训练的网络模型可能会很大（比如，inception，resnet 等），参数很多，而且部署端的机器性能存在差异，就会导致推理速度慢，延迟高。这对于那些高实时性的应用场合是致命的，比如自动驾驶要求实时目标检测，目标追踪等。所以为了提高部署推理的速度，出现了很多轻量级神经网络，比如 squeezenet，mobilenet，shufflenet 等。基本做法都是基于现有的经典模型提出一种新的模型结构，然后用这些改造过的模型重新训练，再重新部署。

而 TensorRT 则是对训练好的模型进行优化。TensorRT 就只是推理优化器。当你的网络训练完之后，可以将训练模型文件直接丢进 TensorRT中，而不再需要依赖深度学习框架（Caffe，TensorFlow 等）

**可以认为 TensorRT 是一个只有前向传播的深度学习推理框架，这个框架可以将Caffe，TensorFlow，PyTorch 等网络模型解析，然后与 TensorRT 中对应的层进行一一映射，把其他框架的模型统一全部转换到 TensorRT 中，然后在 TensorRT 中可以针对 NVIDIA 自家 GPU 实施优化策略，并进行部署加速。**

TensorRT依赖于Nvidia的深度学习硬件环境，可以是GPU也可以是DLA，如果没有的话则无法使用。TensorRT支持目前大部分的神经网络Layer的定义，同时提供了API让开发者自己实现特殊Layer的操作。

TensorRT 基于 CUDA，NVIDIA 的并行编程模型，能够利用 CUDA-X AI 中的库、开发工具和技术，为人工智能、自动机器、高性能计算和图形优化所有深度学习框架的推理。

TensorRT的部署分为两个部分：

1. 优化训练好的模型并生成计算流图
2. 使用TensorRT Runtime部署计算流图

TensorRT的部署流程：

![](https://pics0.baidu.com/feed/3b87e950352ac65cd5e1c02b4617f91891138af5.jpeg@f_auto?token=30908a1c4e1e018e68b4b5fe68d913eb)

TensorRT的模型导入流程：

![](https://pics6.baidu.com/feed/b3fb43166d224f4aa822e448cd12db5b9922d125.jpeg@f_auto?token=5ebfd55ec0ce398da0190460f29f21cd)

TensorRT的优化过程：

![](https://pics1.baidu.com/feed/4e4a20a4462309f72aafe27acfeb47fad5cad681.jpeg@f_auto?token=2048eb6e71d6a01d744bccf83dc96a10)

网络模型在导入至TensorRT后会进行一系列的优化。

TensorRT官网下载地址：https://developer.nvidia.com/zh-cn/tensorrt

开发者指南：https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html

Github地址：https://github.com/NVIDIA/TensorRT

[TensorRT入门](https://zhuanlan.zhihu.com/p/371239130)

[TensorRT介绍](https://blog.csdn.net/weixin_42111770/article/details/114336102)

### 1.2 推理服务器Triton（NVIDIA,网络相关的服务类应用）

推理识别是人工智能最重要的落地应用，其他与深度学习相关的数据收集、标注、模型训练等工作，都是为了得到更好的最终推理性能与效果。

几乎每一种深度学习框架都能执行个别的推理工作，包括 Tensorflow、Pytorch、MXNet 等通用型框架与 YOLO 专属的 Darknet 框架，此外还有 ONNX 开发推理平台、NVIDIA TensorRT 加速推理引擎，也提供推理相关的 C / C++ 与 Python 开发接口，这是大部分技术人员所熟悉的方法。

在垂直应用方面，NVIDIA 的 **DeepStream** 智能分析工具是非常适合用在种类固定且需要长期统计分析的场景，包括各种交通场景的人 / 车流量分析、工业流水线质量检测等应用，并且在早期视觉（Visualization）类推理功能之上，再添加对话（Conversation）类推理功能，让使用范围更加完整。

上述的推理方式通常适合在识别固定种类与固定输入源的使用场景，在交通、工业自动化领域、无人设备等领域的使用比较普及。

**但是这种方式并不适合在网络相关的服务类应用中使用** ，包括在线的产品推荐、图像分类、聊天机器人等应用， **因为在线服务需要同时面对未知数量与类型的数据源，并且透过 HTTP 协议进行数据传输的延迟问题** ，也是严重影响用户体验感的因素，这是绝大部分网路服务供应商要导入 AI 智能识别技术所面临的共同难题。

**NVIDIA Triton** 推理服务器的最大价值，便是 **为服务类智能应用提供一个完整的解决方案** ，因此首先需要解决以下的三大关键问题：

**1. 高通用性：**

(1) 广泛支持多种计算处理器：包括具备 NVIDIA GPU 的 x86 与 ARM CPU 设备，也支持纯 CPU 设备的推理计算。

(2) 广泛支持各种训练框架的文件格式：包括 TensorFlow 1.x/2.x、PyTorch、ONNX、TensorRT、RAPIDS FIL（用于 XGBoost、Scikit-learn Random Forest、LightGBM）、OpenVINO、Python 等。

(3) 广泛支持各种模型种类：包括卷积神经网络 (CNN)、循环神经网络 (RNN)、决策树、随机森林和图神经网络等算法。

**2.部署便利：**

(1) 可在横向扩展的云或数据中心、企业边缘，甚至 NVIDIA Jetson 等嵌入式设备上运行。

(2) 支持用于 AI 推理的裸机和虚拟化环境，包括 VMware vSphere 与基于 Docker 技术的 Kubernetes 管理机制。

(3) 可托管于多种人工智能云平台，包括 Amazon SageMaker、Azure ML、Google Vertex AI、阿里巴巴 AI、腾讯 TI-EMS 等平台。

**3.性能优化：**

(1) **动态批量处理** ：推理优化的一个因素是批量大小，或者您一次处理多少个样本，GPU 以更高的批量提供高吞吐量。然而，对于实时应用程序，服务的真正限制不是批量大小甚至吞吐量，而是为最终客户提供出色体验所需的延迟。

(2) **模型并发执行** ：GPU 是能够同时执行多个工作负载的计算设备，NVIDIA Triton 推理服务器通过在 GPU 上同时运行多个模型来最大限度地提高性能并减少端到端延迟，这些模型可以是相同的，也可以是来自不同框架的不同模型。GPU 内存大小是同时运行模型数量的唯一限制，这会影响GPU利用率和吞吐量。

[NVIDIA Triton系列文章（1）：应用概论](https://zhuanlan.zhihu.com/p/577131256)

https://developer.nvidia.cn/zh-cn/nvidia-triton-inference-server

https://github.com/triton-inference-server/server

### 1.3 NCNN/TNN(腾讯)

#### 1 NCNN（手机端）

NCNN是腾讯优图实验室首个开源项目，是一个为手机端极致优化的高性能神经网络前向计算框架。并在2017年7月正式开源。NCNN做为腾讯优图最“火”的开源项目之一，是一个为手机端极致优化的高性能神经网络前向计算框架，在设计之初便将手机端的特殊场景融入核心理念，是业界首个为移动端优化的开源神经网络推断库。能实现无第三方依赖，跨平台操作，在手机端CPU运算速度在开源框架中处于领先水平。基于该平台，开发者能够轻松将深度学习算法移植到手机端，输出高效的执行，进而产出人工智能APP，将AI技术带到用户指尖。

NCNN从设计之初深刻考虑手机端的部署和使用。无第三方依赖，跨平台，手机端 CPU的速度快于目前所有已知的开源框架。基于 NCNN，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP，将 AI 带到你的指尖。NCNN目前已在腾讯多款应用中使用，如 QQ，Qzone，微信，天天P图等。

下面是NCNN在各大系统平台的应用发展状态情况：

![](https://pics0.baidu.com/feed/d50735fae6cd7b89012233e4b1c109aed8330e7d.jpeg@f_auto?token=e956710da06ddaa90696d2d0a71dda12)

从NCNN的发展矩阵可以看出，NCNN覆盖了几乎所有常用的系统平台，尤其是在移动平台上的适用性更好，在Linux、Windows和Android、以及iOS、macOS平台上都可以使用GPU来部署模型。

根据官方的功能描述，NCNN在各方面的性能都比较优良：

l 支持卷积神经网络，支持多输入和多分支结构，可计算部分分支

l 无任何第三方库依赖，不依赖 BLAS/NNPACK 等计算框架

l 纯 C++ 实现，跨平台，支持 android ios 等

l ARM NEON 汇编级良心优化，计算速度极快

l 精细的内存管理和数据结构设计，内存占用极低

l 支持多核并行计算加速，ARM big.LITTLE cpu 调度优化

l 支持基于全新低消耗的 vulkan api GPU 加速

l 整体库体积小于 700K，并可轻松精简到小于 300K

l 可扩展的模型设计，支持 8bit 量化 和半精度浮点存储，可导入 caffe/pytorch/mxnet/onnx/darknet/keras/tensorflow(mlir) 模型

l 支持直接内存零拷贝引用加载网络模型

l 可注册自定义层实现并扩展

l 恩，很强就是了，不怕被塞卷 QvQ

除此之外，NCNN在对各种硬件设备的支持上也非常给力：

![](https://pics2.baidu.com/feed/bf096b63f6246b60d6fee1c5561d5145530fa2fa.jpeg@f_auto?token=d7c9a1ea55b036d1da0cb1f7600f8986)

NCNN的官方代码地址：https://github.com/Tencent/ncnn

#### 2 TNN(移动端)

开源地址：https://github.com/Tencent/TNN

轻量级部署，TNN 助力深度学习提速增效

深度学习对算力的巨大需求一直制约着其更广泛的落地，尤其是在移动端：手机处理器性能弱、算力无法多机拓展、运算耗时长等因素常常导致发热和高功耗，并直接影响 app 等应用的用户体验。

腾讯优图基于自身在深度学习方面的技术积累，借鉴业内主流框架的优点，推出了针对手机端的高性能、轻量级移动端推理框架 TNN。

TNN 在设计之初便将「移动端」、「高性能」融入核心理念，对 2017 年开源的 ncnn 框架进行了重构升级。通过 GPU 深度调优、ARM SIMD 深入汇编指令调优、低精度计算等技术手段，TNN 在性能上取得了进一步提升。以下是 MNN、ncnn、TNN 框架在多款主流平台上的实测性能：

![](https://pics2.baidu.com/feed/21a4462309f790520fcefece419b1acc79cbd5d8.jpeg@f_auto?token=c89ffb150d94beb260d8ef8f071e2c93)

![](https://pics7.baidu.com/feed/500fd9f9d72a6059cfe99fff665cf99d013bbacd.jpeg@f_auto?token=9adabb6bfb16f7cae5a9490d6c94500a)

![](https://pics2.baidu.com/feed/a8014c086e061d951edff0a2369cc7d763d9ca7a.jpeg@f_auto?token=40c75c434475b66fa9ec8e2409e43336)

![](https://pics1.baidu.com/feed/8ad4b31c8701a18b0580507fd047ca0e2838fe02.jpeg@f_auto?token=f24516e926e957f1dd72b8122f3b9fc8)

**TNN 在麒麟 970、骁龙 835、骁龙 845、骁龙 615 平台上的实测性能数据。注：纵轴单位：ms 测试分支：MNN:1.0.0(2020.05.07), ncnn:20200413, TNN: master(2020.06.10)。测试模型：**https://github.com/alohali/benchmark-models

低精度计算对 TNN 的性能提升起到了重要作用。

在神经网络计算中，浮点精度在许多研究和业务落地成果中都被证明存在一定冗余，而在计算、内存资源都极为紧张的移动端，消除这部分冗余变得极为必要。TNN 引入了 INT8、 FP16、 BFP16 等多种计算低精度的支持，相比大部分仅提供 INT8 支持的框架，TNN 不仅能灵活适配不同场景，还让计算性能大大提升。

TNN 通过采用 8bit 整数代替 float 进行计算和存储，使模型尺寸和内存消耗均减少至 1/4，计算性能提升 50% 以上。此外，TNN 还引入 arm 平台 BFP16 的支持。相比浮点模型，BFP16 使模型尺寸、内存消耗减少 50%，在中低端机上的性能提升约 20%。骁龙 615 平台实测结果如下所示：

![](https://pics5.baidu.com/feed/b8389b504fc2d5620292012ca9795de974c66ccc.jpeg@f_auto?token=ef38175cc75ec96cd73576daeb031dff)

![](https://pics3.baidu.com/feed/dcc451da81cb39db2eff75839e7ec422aa183092.jpeg@f_auto?token=c066fa1d951390ec53989cfac2f16e66)

**通用、轻便是 TNN 框架的另一大亮点。**长久以来，不同框架间的模型转换是 AI 项目应用落地的痛点。TNN 设计了与平台无关的模型表示，为开发人员提供统一的模型描述文件和调用接口，支持主流安卓、iOS 等操作系统，适配 CPU、 GPU、NPU 硬件平台。

企业凭借一套流程即可部署到位，简单易用、省时省力。同时，TNN 通过 ONNX 可支持 TensorFlow、PyTorch、MXNet、Caffe 等多种训练框架。TNN 目前支持的 ONNX 算子超过 80 个，覆盖主流 CNN 网络。TNN 所有算子均为源码直接实现，不依赖任何第三方，且接口易用，切换平台时仅需修改调用参数即可。

### 1.4 OpenVino(Intel)

OpenVINO工具套件全称是**O**pen **V**isual **I**nference & **N**eural Network **O**ptimization，是Intel于2018年发布的， **开源、商用免费** 、主要应用于计算机视觉、实现神经网络模型优化和推理计算(Inference)加速的软件工具套件。由于其商用免费，且可以把深度学习模型部署在英尔特CPU和集成GPU上，大大节约了显卡费用，所以越来越多的深度学习应用都使用OpenVINO工具套件做深度学习模型部署。

OpenVINO是一个Pipeline工具集，同时可以兼容各种开源框架训练好的模型，拥有算法模型上线部署的各种能力，只要掌握了该工具，你可以轻松的将预训练模型在Intel的CPU上快速部署起来。

对于AI工作负载来说，OpenVINO提供了深度学习推理套件（DLDT)，该套件可以将各种开源框架训练好的模型进行线上部署，除此之外，还包含了图片处理工具包OpenCV，视频处理工具包Media SDK，用于处理图像视频解码，前处理和推理结果后处理等。

在做推理的时候，大多数情况需要前处理和后处理，前处理如通道变换，取均值，归一化，Resize等，后处理是推理后，需要将检测框等特征叠加至原图等，都可以使用OpenVINO工具套件里的API接口完成。

OpenVino目前支持Linux、Windows、macOS、Raspbian等系统平台。

![](https://pics0.baidu.com/feed/242dd42a2834349b323b4f890d0f5ec737d3beef.jpeg@f_auto?token=aec96dcaf69a1e9c03e87883d6ae1098)

**OpenVINO工具套件** **主要包括：** Model Optimizer(模型优化器)——用于优化神经网络模型的工具，Inference Engine(推理引擎)——用于加速推理计算的软件包。

![](https://pics4.baidu.com/feed/00e93901213fb80e43b8685289346427b838943c.jpeg@f_auto?token=078fc24b610382ff85ef46331734a2ab)

模型优化器是一个python脚本工具，用于将开源框架训练好的模型转化为推理引擎可以识别的中间表达，其实就是两个文件，xml和bin文件，前者是网络结构的描述，后者是权重文件。模型优化器的作用包括压缩模型和加速，比如，去掉推理无用的操作(Dropout)，层的融合(Conv + BN + Relu)，以及内存优化。

推理引擎是一个支持C\C++和python的一套API接口，需要开发人员自己实现推理过程的开发，开发流程其实非常的简单，核心流程如下：

l 装载处理器的插件库

l 读取网络结构和权重

l 配置输入和输出参数

l 装载模型

l 创建推理请求

l 准备输入Data

l 推理

l 结果处理

OpenVino工具套件的工作流程图：

![](https://pics0.baidu.com/feed/d000baa1cd11728b87604edf0c1988c7c1fd2ce9.jpeg@f_auto?token=63445328b92e67bc8d2a9a170e4b2c25)

OpenVino的官方地址：https://docs.openvinotoolkit.org/latest/index.html

### 1.5 MediaPipe(Google)

MediaPipe 是一款由 Google Research 开发并开源的多媒体机器学习模型应用框架。在谷歌，一系列重要产品，如 YouTube、Google Lens、ARCore、Google Home 以及 Nest，都已深度整合了 MediaPipe。

MediaPipe 是一个基于图形的跨平台框架，用于构建多模式（视频，音频和传感器）应用的机器学习管道。MediaPipe 可在移动设备、工作站和服务器上跨平台运行，并支持移动 GPU 加速。使用 MediaPipe，可以将应用的机器学习管道构建为模块化组件的图形。MediaPipe 不仅可以被部署在服务器端，更可以在多个移动端 （安卓和苹果 iOS）和嵌入式平台（Google Coral 和树莓派）中作为设备端机器学习推理 （On-device Machine Learning Inference）框架。

![](https://pics0.baidu.com/feed/377adab44aed2e73a34652fe3be4ea8285d6faa0.jpeg@f_auto?token=8e65a91d6b3cdde3d34cd6b1985c12d5)

一款多媒体机器学习应用的成败除了依赖于模型本身的好坏，还取决于设备资源的有效调配、多个输入流之间的高效同步、跨平台部署上的便捷程度、以及应用搭建的快速与否。

基于这些需求，谷歌开发并开源了 MediaPipe 项目。除了上述的特性，MediaPipe 还支持 TensorFlow 和 TF Lite 的推理引擎（Inference Engine），任何 TensorFlow 和 TF Lite 的模型都可以在 MediaPipe 上使用。同时，在移动端和嵌入式平台，MediaPipe 也支持设备本身的 GPU 加速。

MediaPipe 专为机器学习（ML）从业者而设计，包括研究人员，学生和软件开发人员，他们实施生产就绪的 ML 应用程序，发布伴随研究工作的代码，以及构建技术原型。MediaPipe 的主要用例是使用推理模型和其他可重用组件对应用机器学习管道进行快速原型设计。MediaPipe 还有助于将机器学习技术部署到各种不同硬件平台上的演示和应用程序中。

MediaPipe 的核心框架由 C++ 实现，并提供 Java 以及 Objective C 等语言的支持。MediaPipe 的主要概念包括数据包（Packet）、数据流（Stream）、计算单元（Calculator）、图（Graph）以及子图（Subgraph）。数据包是最基础的数据单位，一个数据包代表了在某一特定时间节点的数据，例如一帧图像或一小段音频信号；数据流是由按时间顺序升序排列的多个数据包组成，一个数据流的某一特定时间戳（Timestamp）只允许至多一个数据包的存在；而数据流则是在多个计算单元构成的图中流动。MediaPipe 的图是有向的——数据包从数据源（Source Calculator或者 Graph Input Stream）流入图直至在汇聚结点（Sink Calculator 或者 Graph Output Stream） 离开。

![](https://pics1.baidu.com/feed/d0c8a786c9177f3e5217be81b52a70ce9e3d560d.jpeg@f_auto?token=a9eaa07c36f8e834d24c104df4d110bf)

MediaPipe 在开源了多个由谷歌内部团队实现的计算单元（Calculator）的同时，也向用户提供定制新计算单元的接口。创建一个新的 Calculator，需要用户实现 Open()，Process()，Close() 去分别定义 Calculator 的初始化，针对数据流的处理方法，以及 Calculator 在完成所有运算后的关闭步骤。为了方便用户在多个图中复用已有的通用组件，例如图像数据的预处理、模型的推理以及图像的渲染等， MediaPipe 引入了子图（Subgraph）的概念。因此，一个 MediaPipe 图中的节点既可以是计算单元，亦可以是子图。子图在不同图内的复用，方便了大规模模块化的应用搭建。

MediaPipe不支持除了tensorflow之外的其他深度学习框架，但是对各种系统平台和语言的支持非常友好：

![](https://pics4.baidu.com/feed/8644ebf81a4c510f70bd4c1bdebc6e24d52aa5a1.jpeg@f_auto?token=5c3ed7fa6a2c0bfa4eeba204a30aaa75)

MediaPipe的官方地址：https://google.github.io/mediapipe/

GitHub地址：https://github.com/google/mediapipe

### 1.6 TVM(华盛顿大学)

TVM是一款开源的、端到端的深度学习模型编译框架，用于优化深度学习模型在CPU、GPU、ARM等任意目标环境下的推理运行速度，常见的应用场景包括：

* 需要兼容所有主流模型作为输入，并针对任意类型的目标硬件生成优化部署模型的场景
* 对部署模型的推理延迟、吞吐量等性能指标有严格要求的场景
* 需要自定义模型算子、自研目标硬件、自定义模型优化流程的场景

![](https://pic3.zhimg.com/v2-8ff1a3ed506955415b0953fb9e6936e2_r.jpg)

TVM软件框架（图片来自TVM官方网站）

TVM框架如上图：主流的深度学习框架（Tensorflow, Pytorch, MXNet等）导出的模型作为TVM框架的输入，经过该框架内一系列的图优化操作以及算子级的自动优化操作后最终转化为针对目标运行时（CPU/GPU/ARM等）的部署模型，优化后的模型理论上可以最大化地利用目标硬件的资源以最小化模型的推理延迟。

**为什么用TVM优化模型推理？**

模型推理场景下用于模型优化、部署的软件框架仍处于“百家争鸣”的状态，其原因在于推理任务的复杂性：训练后的模型需要部署于多样的设备上（Intel CPU/ NVGPU/ ARM CPU/FPGA/ AI芯片等），要在这些种类、型号不同的设备上都能保证模型推理的高效是一项极有挑战的工作。

一般来说，主流的硬件厂商会针对自家硬件推出对应的推理加速框架以最大化利用硬件性能，如Intel的OpenVINO、ARM的ARM NN、Nvidia的TensorRT等，但这些框架在实际应用场景中会遇到不少问题：

* 厂商推理框架对主流训练框架产生的模型的算子种类支持不全，导致部分模型无法部署
* 模型部署侧的开发人员需要针对不同的硬件编写不同的框架代码，花精力关注不同框架对算子的支持差异和性能差异等

因此，一套可以让我们在任意硬件上高效运行任意模型的统一框架就显得尤其有价值，而TVM正是这样一套框架。

**TVM如何优化模型推理？**

实 际 上，“运 行 模 型/代码 到 任 意 种 类 的 硬件”并不是一个概念上全新的课题。在计算机编程语言发展的早期阶段（第二代编程语言），人们也曾遇到过类似的困境，即一种硬件平台必须配套一种汇编语言且不同汇编语言无法跨平台运行的情况。随着该领域的发展，人们给出了解决之道——引入高级语言和编译器，如下图：

![](https://pic3.zhimg.com/v2-bed921e35cac343404588c56c282582e_r.jpg)

汇编语言向高级语言-编译器路线发展

* 程序员负责用高级语言描述上层业务逻辑，不必关注具体硬件特性
* 编译器将高级语言逐层转化为更底层的符号，也称中间表示（IR），其中最底层的IR可以对接不同的硬件，进而转化为针对不同目标的机器码

TVM框架正是借鉴了这种思想，我们可以把TVM理解成一种广义的“编译器”：TensorFlow、PyTorch等训练框架导出的模型可以认为是“高级语言”，而TVM内部的图级别表达式树、算子级的调度Stages则可以认为是“高级语言”的“中间表示”，如下图。

![](https://pic1.zhimg.com/v2-949ec28eae84d1300355851086ccfbd4_r.jpg)

深度学习模型“编译器”

**TVM框架构成**

以“模型部署”为边界，TVM可以分为TVM编译器和TVM运行时两个组件：

![](https://pic3.zhimg.com/v2-35e0634e49b50194b8f3fe544bcbc086_r.jpg)

TVM组件：编译器和运行时（图片来自TVM官方网站）

编译器负责模型的编译和优化，是TVM的主体功能：

* 编译优化过程支持Python和C++接口
* 系统环境支持Linux、Windows以及MacOS平台（部分功能如AutoTVM在非Linux平台可能受限）

运行时负责在目标设备上执行编译器生成的模型推理代码：

* 部署过程支持JS, Java, Python, C++语言
* 部署平台除了支持Linux、Windows以及MacOS系统，还支持Android, IOS, 树莓派等端侧系统

**TVM编译过程**

TVM编译器是TVM的主体功能组件，负责优化、编译深度学习模型为可在目标设备运行推理任务的代码，整体编译过程如下图所示：

![](https://pic3.zhimg.com/v2-e9ca8bb7c81565d74b4a4869e9201e06_r.jpg)

TVM编译栈（图片来自TVM官方网站）

上图中，蓝色方框代表TVM编译过程中涉及到的数据结构，黄色方框代表处理这些数据结构的算法，粉色的AutoTVM则是辅助Schedule功能选择参数的一种特殊算法。

整个TVM编译栈的数据结构包括：

* **Model from Frameworks: **从PyTorch、MXNet等框架导出的模型
* **IR Module(Relay):** TVM的图级别IR，数据结构为Relay Expression构成的Relay AST，查看源码可以发现，relay.Function实际上是relay.Expr的一种子类，其功能是为Relay Pass（遍历Relay AST的手段）提供一个入口点，故而官网中也把Relay IR称为"End to end function"
* **IR Module(TIR): **TVM的Tensor级别IR，仍然以AST的形式组织Expr，但包含了各算子（对应网络层）的具体调度细节（循环嵌套、并行、指令集等），这一层IR由一系列的tir.PrimFunc组成，此处的PrimFunc不再是访问整个AST的“入口点”，而是对应神经网络的一个层或融合层（如Conv+BN+Activation）的具体计算调度过程
* **Runtime Module: **TVM编译栈的最底层IR，直接对接Runtime在目标设备上运行

TVM编译栈涉及的数据处理算法包括：

* **Frontend: **负责将外部框架生成的神经网络模型转化为TVM的图级别模型Relay IR
* **Relay Passes: **负责在图级别优化IR的计算过程，常见的图优化Pass包括常量折叠、死代码消除、推理简化、内存布局转化、Op融合等
* **Scheduling: **负责转化Relay Passes优化后的Relay IR为TensorIR，大致的流程是对于每个层/融合层对应的算子，执行[http://**graph_runtime_codegen.cc**](http://graph_runtime_codegen.cc/)的Codegen，根据TVM框架中注册的算子compute和schedule函数，将每个算子转化为具体的计算调度过程（*Relay IR* --> **FTVMCompute** --> *Tensors* --> **FTVMSchedule** -->  *Schedule Stages* ）
* **TIR Passes: **负责Tensor级别的IR优化，常见的如access index simplification，也有负责IR lower的function entry decoratation等
* **Target Translation(CodeGen): **设备相关的低级别Codegen，将TIR转化为TVM Runtime所需的Module

**TVM特色**

**特色1：算子的计算和调度过程定义的分离**

TVM的一个特色是借鉴Halide语言的计算和调度分离定义的设计方法，将深度学习模型的算子计算方法定义和具体计算调度方式做了解耦：

•计算（Compute）定义了每个Relay Op的具体计算过程

•调度（Schedule）则细化了计算过程每个具体步骤的实际计算细节

以卷积算子为例，计算（Compute）定义的是算子应如何根据卷积核、卷积步长等参数操作输入Tensor，并输出卷积后的Tensor的过程；调度（Schedule）则定义了更具体的计算细节，例如卷积的6层计算循环的顺序、每层循环是否需要并行计算、并行数设定为多少等。

之所以做这样的分离式设计，是因为一般情况下，特定算子的计算方式是不变的，而算子的调度却有几乎无限种配置方法，显然对于不同的Target如CPU、GPU，其最优调度配置必然大相径庭；即使对于同一种Target如LLVM，不同型号的CPU也会导致最优配置相差甚远。TVM的计算-调度分离的设计方式，给二次开发人员提供了足够便利的API，方便其自定义性能更佳的调度配置。

**特色2：AutoTVM**

默认情况下，TVM为定义的所有算子针对常见的Target（x86/ARM/CUDA等）各提供了一套默认的调度配置。但一方面指定Target的默认配置不可能在所有型号的硬件下最大程度地利用运行环境的计算资源；另一方面调度的可选配置空间巨大，人工手动调整调度效率低且无法保证调度性能。为了解决该问题，TVM开发了AutoTVM模块通过搜索的方式获得具体硬件下的最优调度配置，总体步骤如：

* 针对每个Relay子函数，定义一系列调度原语（Schedule primitives），通过调度原语的组合实现计算结果等价的Relay子函数
* 在海量的配置组合中为每个Relay子函数搜索最优的调度配置

对于卷积这样的计算密集算子，调度配置的搜索空间通常有几十亿种选择，比较棘手的问题就是如何高效的在搜索空间中获得最优/局部最优解。常见的做法有两种：

* 遍历所有可能的参数编译模型并评估
* 评估部分参数模型，训练一个性能评估函数，用于指导搜索过程寻找更好的调度配置

由于搜索空间过于巨大，第一种方法需要花费的时间难以估量；AutoTVM采用的是第二种，用一个XGBoost模型（其他回归预测模型也可）作为评估调度配置性能的模型（CostModel），用以指导模拟退火算法（默认）寻找最优调度配置。

搜索过程完毕后AutoTVM会将过程中记录的最优配置保存下来，编译模型时可用于代替默认的Target调度配置。

### 1. 7 ONNX Runtime(微软)

**ONNX Runtime**  **：由微软推出，用于优化和加速机器学习推理和训练** ，适用于[ONNX模型](https://so.csdn.net/so/search?q=ONNX%E6%A8%A1%E5%9E%8B&spm=1001.2101.3001.7020)，是一个跨平台推理和训练机器学习加速器(ONNX Runtime is a cross-platform inference and training machine-learning accelerator)，源码地址：https://github.com/microsoft/onnxruntime，最新发布版本为v1.11.1，License为MIT：

1.[ONNX](https://so.csdn.net/so/search?q=ONNX&spm=1001.2101.3001.7020) Runtime Inferencing：高性能推理引擎

(1).可在不同的操作系统上运行，包括Windows、Linux、Mac、Android、iOS等；

(2).可利用硬件增加性能，包括CUDA、TensorRT、DirectML、OpenVINO等；

(3).支持PyTorch、TensorFlow等深度学习框架的模型，需先调用相应接口转换为ONNX模型；

(4).在Python中训练，确可部署到C++/Java等应用程序中。

2.ONNX Runtime Training：于2021年4月发布，可加快PyTorch对模型训练，可通过CUDA加速，目前多用于Linux平台。

ONNX 是微软与Facebook和AWS共同开发的深度学习和传统机器学习模型的开放格式。ONNX格式是开放式生态系统的基础，使人工智能技术更易于访问，而且可以为不同人群创造价值：

开发人员可以为自己机器学习任务选择合适的框架，框架作者可以集中精力推出创新，提高框架的性能。对于硬件供应商来说，也可以简化神经网络计算的复杂度，实现优化算法。

过去20多年来，微软一直在进行人工智能研究，并将机器学习和深度神经网络整合到旗下众多产品和服务中。由于团队使用许多不同的培训框架并针对不同的部署选项，因此确实需要统一这些分散的解决方案，以便快速，简单地操作模型。

此次开源的ONNX Runtime为这个问题提供了解决方案。该工具为数据科学家在选择的框架内训练和调整模型提供了更高的灵活性，并在云端和产品设计中以高性能打造这些模型。

![](https://t11.baidu.com/it/app=49&f=JPEG&fm=173&fmt=auto&u=2527732186%2C3616517025?w=360&h=441&s=A162D014CC8E1C4B02A3655C0300B0FA)

**ONNX引擎是Windows ML组件的关键部分。**微软正在将这种机器学习界面纳入到Windows 10系统中，让开发人员在自己的Windows应用程序上使用训练过的机器学习模型。 Windows ML推理引擎可以在Windows设备上对这些模型进行本地评估，开发人员无需将模型放到云中运行。

ONNX Runtime有什么用？ONNX是微软公开推出的首款推理机，完整支持ONNX 1.2和 ONNX机器学习的更高版本。这意味着ONNX Runtime直接随着ONNX的标准进步，实现对一大批AI模型和技术突破的支持。

微软的团队在使用ONNX Runtime来改善在Bing搜索、广告、Office产品套件等核心业务中的评分延时，提高运行效率。而对于由其他工具转而使用ONNX的模型，其平均表现提升了2倍。此外，微软的其他一些服务，如Windows ML 和ML.net等，也纳入了ONNX Runtime支持的业务之中。

ONNX Runtime在设计上是轻量级和模块化的，CPU的构建只有几M字节。可扩展架构、优化硬件加速器，降低计算延时，提升计算效率。

英伟达、英特尔、高通等巨头纷纷支持

很多业界领先的企业都在积极将自己的技术与ONNX Runtime实现集成和整合，使自己的服务能够完整支持ONNX规范，同时实现性能的最优化。

微软和英特尔正在合作，将nGraph编译器集成为ONNX Runtime的执行提供程序。nGraph编译器能够通过应用非设备特定和设备特定的优化来实现现有和未来的硬件加速。与原生框架相比，使用nGraph Compiler执行CPU推理任务可将性能提升45倍。

英伟达正在努力将TensorRT与ONNX Runtime实现整合，为在英伟达的GPU上部署快速增长的模型和应用程序提供简便的工作流程，同时实现最佳性能。

NVIDIA TensorRT包括一个高性能的推理优化器和运行时，可以在推荐器，自然语言处理和图像/视频处理等应用程序中以最小的延迟提供极高的吞吐量。

ONNX的另一个早期支持者是高通，也支持ONNX Runtime。高通AI业务高级主管Gary Brotman表示：“ONNX Runtime的推出和开源是微软提升跨设备类别框架的互操作性，实现标准化和性能优化的积极一步，我们希望开发人员欢迎在骁龙移动平台上支持ONNX Runtime。

业内领先的物联网芯片制造商恩智浦（NXP）最近也宣布支持ONNX Runtime。“我们希望，用户在众多机器学习框架中进行选择时能够拥有最大的灵活性和自由度。”

恩智浦AI技术中心负责人Markus Levy表示。“我们很高兴能够通过支持微软在我们的平台上发布ONNX Runtime，让机器学习开发者客户群能够享受ONNX带来的好处。”

如何使用ONNX Runtime

首先，你需要建立一个ONNX模型。没有ONNX模型？没问题。ONNX的优点就是能够通过众多工具实现框架的互操作性 。你可以直接从ONNX Model Zoo获得ResNet和TinyYOLO等热门模型的预训练版本。

**你还可以使用Azure Custom Vision Cognitive Service创建自己的自定义计算机视觉模型。 如果已经拥有TensorFlow、Keras、Scikit-Learn或CoreML格式的模型，可以使用我们的开源转换器（ONNX MLTools和TF2 ONNX）进行转换。**可以使用Azure机器学习服务训练新模型并保存为ONNX格式。**要使用ONNX Runtime，只需安装所需平台和所选语言的软件包，或者从源代码创建。ONNX Runtime支持 CPU和 GPU（CUDA） 以及兼容Linux、Windows和Mac上的Python、C＃和C接口。具体的安装说明可以在GitHub中获取。**你可以直接从源或预编译的二进制文件将ONNX Runtime集成到代码中，一种简单的操作方法是使用Azure机器学习为应用程序部署服务，以待调用。

参考链接：

https://www.zdnet.com/article/microsoft-open-sources-the-inference-engine-at-the-heart-of-its-windows-machine-learning-platform/

Github：

https://github.com/microsoft/onnxruntime

### 1.8 MNN(alibaba,移动端)

在经过充分的行业调研后，阿里淘系技术部认为当时的推理引擎如TFLite不足以满足手机淘宝这样一个亿级用户与日活的超级App。

于是，他们从零开始自己搭建了属于阿里巴巴的推理引擎MNN。1年前，MNN在Github上开源，截止目前获得了3.9k Stars。

MNN比其他的推理引擎更快更轻量，更符合手机淘宝这样庞大、复杂的生产部署环境。今年3月份，基于MNN的引擎设计与优化理念，阿里在MLSys 2020上发表了论文，并进行了oral presentation。

开源1年以来，基于阿里内外的用户反馈和业务推动，他们称，MNN在许多方面都取得了长足的进步：

在阿里巴巴集团内部得到广泛推广，成为了端上推理引擎的事实标准，覆盖了如手机淘宝、手机天猫、优酷、钉钉、闲鱼等20多个App。新添了模型训练的支持，从此MNN不再是单纯的推理引擎，而是具有推理+训练能力的深度学习引擎。基于MNN的训练能力，可以进行Quantization Aware Training (QAT)。在MobileNet上，MNN量化训练之后的模型准确率几乎不降。持续投资于异构硬件后端的优化，尤其是利用ARMv8.2指令集，获得了两倍的性能提升。进一步完善Python工具链，累计新增超过150个接口。开源了应用层开箱即用的解决方案MNNKit，包含了人脸跟踪与检测、人像分割、手势识别场景的解决方案。近日，MNN发布了1.0.0正式版本。相较于0.2.2版本，1.0.0版本的主要升级在于：模型训练、异构性能和Python工具链。

![](https://pics2.baidu.com/feed/63d9f2d3572c11df92253a94f89fc0d6f503c2d4.jpeg@f_auto?token=d807ae400866f7856fac867ff53cef66)

**模型训练**

**模型构建**

MNN支持使用 Express （表达式）接口来构建模型，模型的构建、训练和保存具体可以参考说明文档。

VARP x = inputs[0];x = conv1->forward(x);x = _MaxPool(x, {2, 2}, {2, 2});x = conv2->forward(x);x = _MaxPool(x, {2, 2}, {2, 2});x = _Convert(x, NCHW);x = _Reshape(x, {0, -1});x = ip1->forward(x);x = _Relu(x);x = dropout->forward(x);x = ip2->forward(x);x = _Softmax(x, 1);return {x};据介绍，以MNIST数据集 + Lenet网络为例，一个epoch 60000张图片，一般可达到97-98%的准确率。性能上，同款MBP上，MNN比PyTorch和Caffe都有明显优势；而手机上，MNN也达到了完全可用的性能水准。

![](https://pics7.baidu.com/feed/f636afc379310a55089c16df17fde1af832610a4.jpeg@f_auto?token=20244d86ba2c767d6237e4c0cfd76c40)

**量化训练**

模型量化既可以降低模型大小，又可以利用硬件特性提升推理性能，可谓业务应用必备之选。但美中不足之处在于，模型量化会带来一定的精度损失 —— 对于精度攸关的项目，就难免要做出艰难的选择。

![](https://pics0.baidu.com/feed/03087bf40ad162d9efb39365b3670bea8b13cdad.jpeg@f_auto?token=89b7b64d564dc46b36ac6e6b93a60e7f)

为此，MNN借助自身模型训练能力，实现了模型训练量化，具体实现可以参考说明文档。精度和压缩率方面，以MobileNet V2为例说明：

![](https://pics4.baidu.com/feed/d439b6003af33a87d539d49d58e4b23e5243b524.png@f_auto?token=9c499b2cb95ee79e2791d56ef6bb078d)

注1：训练和验证均采用ImageNet数据集。训练采用32为batch size，执行100个迭代，即，使用了3200张图片进行训练；精度验证则使用了50000张图片。

注2：原始模型为TensorFlow官方模型，官方准确率为71.8%，但因预处理代码上有细微差别，我们测试原始模型的准确率结果稍高于官方；

可以看出，在实现了73%模型尺寸压缩的情况下，量化模型的精度甚至要稍高于原始模型。

![](https://pics3.baidu.com/feed/9f2f070828381f306d6fe0f735b9ee0e6f06f0bd.jpeg@f_auto?token=0bedb22562c5c98329a862e7ed52786a)

异构性能

x86

在x86上，我们重点优化了矩阵乘法。在分析过AVX和Arm向量乘指令差异后，我们修改了AVX下的权重矩阵布局，降低了I/O布局，以充分利用CPU算力。

此外，我们允许在支持FMA扩展的设备上，启用扩展，将乘法和加法合为一条指令，以进一步降低指令耗时。

当前，FMA扩展的启用开关还放置在CMakeLists.txt中，后续会在运行时判别。

![](https://pics4.baidu.com/feed/79f0f736afc3793172458e5e497c154342a911d2.jpeg@f_auto?token=6bbe1ec2660297697490646c8f7c2905)

综合两项优化，x86上有30%左右的性能优化。

ARM64

在ARM64上，他们面向中低端设备，调整了矩阵乘法的分块策略，矩阵中每个元素的均摊I/O降低了22%；同时，将数据对齐从32字节调整为64字节，与ARM架构CPU下场景的L1 cacheline匹配；最后，优化了缓存预取。优化结果如下：

![](https://pics7.baidu.com/feed/21a4462309f7905278dd6b4aae4b75cc79cbd5c7.jpeg@f_auto?token=251a6f47c7f0ddcfe58dc0c28cfeec06)

ARMv8.2

ARM在「Bringing Armv8.2 Instructions to Android Runtime」一文中，列举了可以在Android运行时中应用的ARMv8.2新特性。其中，FP16 extensions和Dot Product可以分别应用于浮点计算加速和量化计算加速。

FP16 extensions

![](https://pics2.baidu.com/feed/5fdf8db1cb13495474bc005ef2f6305ed0094aad.jpeg@f_auto?token=ded65a421ed078e1dccb07aaf305a2a4)

亦记作asimdhp(Advanced SIMD Half Precision)，是ARMv8.2架构的可选扩展。asimdhp可用时，可以使用相关SIMD指令实现float16的读写计算。float16将float32所需的位数降低了一半，因此在SIMD下，可以实现两倍的并发吞吐，从而优化性能。为此，我们在卷积中，采用[N,C/8,H,W,8]的数据布局，新增了部分卷积实现，效果如下：

![](https://pics4.baidu.com/feed/6a600c338744ebf8fc134c177f41752c6159a7f3.jpeg@f_auto?token=a2bcdc90c53d1d47654ab9b6c8e6eb1d)

精度上几乎没有下降，但是性能足足提升了一倍。搭配上MNN转换工具的--fp16输出选项，模型大小还能减小一半。一箭双雕。

Dot Product

![](https://pics0.baidu.com/feed/b17eca8065380cd7d06a71a601fc0f3258828140.jpeg@f_auto?token=bc1e515f5bd362f8b2727021fae0c2c2)

亦记作asimddp(Advanced SIMD Dot Product)，是ARMv8.2架构的可选扩展。asimddp可用时，可以使用SDOT/UDOT指令实现int8/uint8的点积计算。SDOT/UDOT指令如上图所示，一次可以处理两个4x4 int8/uint8数据乘，并累加到4x1的int32/uint32的寄存器上。这样强大的硬件加速指令，还是双发射的。

![](https://pics4.baidu.com/feed/cf1b9d16fdfaaf51bc504fa228ec36e8f11f7abf.jpeg@f_auto?token=3fa40e9339a48409b882616f80935a95)

实战表现效果也非常明显，在原先int8无法发挥效用的设备上，ARMv8.2也成功实现了耗时减半。

![](https://pics3.baidu.com/feed/79f0f736afc37931332c3725717c154342a911b0.jpeg@f_auto?token=ba342eb0242bb038d19cb8fc5fb8fd94)

Python工具链

2019年的绿盟开发者大会上，我们发布了MNN的Python前端和Python版的转换、量化、可视化工具。而今，Python又增加了对MNN Express (表达式)、模型训练的封装，累计新增超过150个接口。

### 1.9 MACE(小米，移动端）

MACE 是小米公司自研的移动端[深度学习框架](https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6&spm=1001.2101.3001.7020)  **Mobile AI Compute Engine** ，2017年12月15日于公司内部正式发布。2018年6月28日，在“2018（第十三届）开源中国开源世界高峰论坛”上，小米公司人工智能与[云平台](https://so.csdn.net/so/search?q=%E4%BA%91%E5%B9%B3%E5%8F%B0&spm=1001.2101.3001.7020)副总裁崔宝秋博士宣布开源 MACE，赋能中国 AI 产业，以期推动移动端深度学习的发展。

资料

官方手册：[https://buildmedia.readthedocs.org/media/pdf/mace/latest/mace.pdf](https://buildmedia.readthedocs.org/media/pdf/mace/latest/mace.pdf "https://buildmedia.readthedocs.org/media/pdf/mace/latest/mace.pdf")

开源github地址：[https://github.com/XiaoMi/mace.git](https://github.com/XiaoMi/mace.git "https://github.com/XiaoMi/mace.git")

mace模型库地址：[https://github.com/XiaoMi/mace-models.git](https://github.com/XiaoMi/mace-models.git "https://github.com/XiaoMi/mace-models.git")

LOGO：

![](https://img-blog.csdnimg.cn/f5bdfb07dc214568bde98bd08b7434aa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAcGFwYW9mZG91ZG91,size_19,color_FFFFFF,t_70,g_se,x_16)

架构：

![](https://img-blog.csdnimg.cn/9fecc5c0cc984c22a0d9fa25270f6f8d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAcGFwYW9mZG91ZG91,size_20,color_FFFFFF,t_70,g_se,x_16)

简单总结一下：

1.MACE定义了自己的模型格式，提供工具将各类开源模型转为MACE格式。

2.MACE interpreter是用来解释图的。

3.后端支持DSP,CPU,GPU用来对算子加速，和某单位对我们提出的算子层API一个层面的意思。
GITHUB中的后端代码目录如下：

![](https://img-blog.csdnimg.cn/ab0e514e64fe4f02bef9d97c7732eeca.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAcGFwYW9mZG91ZG91,size_20,color_FFFFFF,t_70,g_se,x_16)

部署思路大同小异，不过多解释：

![](https://img-blog.csdnimg.cn/131e01b99d2c4af9b4ad844626ddeb9b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAcGFwYW9mZG91ZG91,size_19,color_FFFFFF,t_70,g_se,x_16)

![](https://img-blog.csdnimg.cn/58939ec7015542039a8ba04fe1d48978.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAcGFwYW9mZG91ZG91,size_20,color_FFFFFF,t_70,g_se,x_16)

MACE SDK的组织方式：

![](https://img-blog.csdnimg.cn/a724ac9ea94b4192a4231037083fe8e7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAcGFwYW9mZG91ZG91,size_20,color_FFFFFF,t_70,g_se,x_16)

模型格式

MACE 自定义了一种类似 Caffe2 的模型格式。Tensorflow 和 Caffe 训练得到的模型可以转换成 MACE 模型进行部署。MACE 采用 YAML 文件描述模型的部署细节。下一章会给出 YAML 文件的创建方法。

模型转换

目前，我们提供了Tensorflow、Caffe模型到MACE模型的转换工具，未来会支持更多的框架。

模型加载

MACE 模型包含两部分：图（model graph）和参数张量（model parameter tensors）。graph 部分使用 Protocol Buffers 做序列化存储。所有的模型参数张量（model parameter tensors）被串联存储在一个连续的字节数组中，在后续段落中我们称之为张量数据（tensor data）。**应该就是类似于darknet那种模型结构和模型权重分开存储的方式，darknet的权重文件也是一个连续的字节数组。**

模型可以通过3种方式进行加载：

1. model graph 和 tensor data 均在外部动态加载（默认情况下，从文件系统加载，但是用户可以自由选择一些实现方式，例如压缩或加密）。这种方式具有最大的灵活性但是模型的保护性最差。
2. model graph 和 tensor data 转换成 C++ 代码并通过执行编译后的代码进行加载。这种方式提供了最强的模型保护措施，是最简单的部署方式。
3. model graph 按照方式 2 一样被转换成 C++ 代码，tensor data 按照方式 1 进行外部加载。

### 1.10 Paddle-Lite(百度)

Paddle Lite 是一个高性能、轻量级、灵活性强且易于扩展的深度学习推理框架，定位于支持包括移动端、嵌入式以及边缘端在内的多种硬件平台。

当前 Paddle Lite 不仅在百度内部业务中得到全面应用，也成功支持了众多外部用户和企业的生产任务。

快速入门

使用 Paddle Lite，只需几个简单的步骤，就可以把模型部署到多种终端设备中，运行高性能的推理任务，使用流程如下所示：

**一. 准备模型**

Paddle Lite 框架直接支持模型结构为 [PaddlePaddle](https://github.com/PaddlePaddle/Paddle) 深度学习框架产出的模型格式。目前 PaddlePaddle 用于推理的模型是通过 [save_inference_model](https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/api/paddle/static/save_inference_model_cn.html#save-inference-model) 这个 API 保存下来的。 如果您手中的模型是由诸如 Caffe、Tensorflow、PyTorch 等框架产出的，那么您可以使用 [X2Paddle](https://github.com/PaddlePaddle/X2Paddle) 工具将模型转换为 PaddlePaddle 格式。

**二. 模型优化**

Paddle Lite 框架拥有优秀的加速、优化策略及实现，包含量化、子图融合、Kernel 优选等优化手段。优化后的模型更轻量级，耗费资源更少，并且执行速度也更快。 这些优化通过 Paddle Lite 提供的 opt 工具实现。opt 工具还可以统计并打印出模型中的算子信息，并判断不同硬件平台下 Paddle Lite 的支持情况。您获取 PaddlePaddle 格式的模型之后，一般需要通过该 opt 工具做模型优化。opt 工具的下载和使用，请参考[模型优化方法](https://www.paddlepaddle.org.cn/lite/develop/user_guides/model_optimize_tool.html)。

**三. 下载或编译**

Paddle Lite 提供了 Android/iOS/x86/macOS 平台的官方 Release 预测库下载，我们优先推荐您直接下载 [Paddle Lite 预编译库](https://www.paddlepaddle.org.cn/lite/develop/quick_start/release_lib.html)，或者从 Release notes 处获取最新的[预编译编译库](https://github.com/PaddlePaddle/Paddle-Lite/releases)。

Paddle Lite 已支持多种环境下的源码编译，为了避免复杂、繁琐的环境搭建过程，我们建议您使用 [Docker 统一编译环境搭建](https://www.paddlepaddle.org.cn/lite/develop/source_compile/docker_env.html) 进行编译。当然，您也可以根据宿主机和目标设备的 CPU 架构和操作系统，在[源码编译](https://www.paddlepaddle.org.cn/lite/develop/source_compile/compile_env.html)中找到相应的环境搭建及编译指南，自行完成编译环境的搭建。

**四. 预测示例**

Paddle Lite 提供了 C++、Java、Python 三种 API，并且提供了相应 API 的完整使用示例:

* [C++ 完整示例](https://www.paddlepaddle.org.cn/lite/develop/user_guides/cpp_demo.html)
* [Java 完整示例](https://www.paddlepaddle.org.cn/lite/develop/user_guides/java_demo.html)
* [Python 完整示例](https://www.paddlepaddle.org.cn/lite/develop/user_guides/python_demo.html)

您可以参考示例中的说明快速了解使用方法，并集成到您自己的项目中去。

针对不同的硬件平台，Paddle Lite 提供了各个平台的完整示例：

* [Android apps](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/android_app_demo.html) [[图像分类]](https://paddlelite-demo.bj.bcebos.com/apps/android/mobilenet_classification_demo.apk) [[目标检测]](https://paddlelite-demo.bj.bcebos.com/apps/android/yolo_detection_demo.apk) [[口罩检测]](https://paddlelite-demo.bj.bcebos.com/apps/android/mask_detection_demo.apk) [[人脸关键点]](https://paddlelite-demo.bj.bcebos.com/apps/android/face_keypoints_detection_demo.apk) [[人像分割]](https://paddlelite-demo.bj.bcebos.com/apps/android/human_segmentation_demo.apk)
* [iOS apps](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/ios_app_demo.html)
* [Linux apps](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/linux_arm_demo.html)
* [Arm](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/arm_cpu.html)
* [x86](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/x86.html)
* [OpenCL](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/opencl.html)
* [Metal](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/metal.html)
* [华为麒麟 NPU](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/huawei_kirin_npu.html)
* [华为昇腾 NPU](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/huawei_ascend_npu.html)
* [昆仑芯 XPU](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/kunlunxin_xpu.html)
* [昆仑芯 XTCL](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/kunlunxin_xtcl.html)
* [高通 QNN](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/qualcomm_qnn.html)
* [寒武纪 MLU](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/cambricon_mlu.html)
* [(瑞芯微/晶晨/恩智浦) 芯原 TIM-VX](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/verisilicon_timvx.html)
* [Android NNAPI](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/android_nnapi.html)
* [联发科 APU](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/mediatek_apu.html)
* [颖脉 NNA](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/imagination_nna.html)
* [Intel OpenVINO](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/intel_openvino.html)
* [亿智 NPU](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/eeasytech_npu.html)

主要特性

* 支持多平台：涵盖 Android、iOS、嵌入式 Linux 设备、Windows、macOS 和 Linux 主机
* 支持多种语言：包括 Java、Python、C++
* 轻量化和高性能：针对移动端设备的机器学习进行优化，压缩模型和二进制文件体积，高效推理，降低内存消耗

持续集成

|              System              | x86 Linux | ARM Linux | Android (GCC/Clang) | iOS |
| :------------------------------: | :-------: | :-------: | :-----------------: | :-: |
|            CPU(32bit)            |          |          |                    |    |
|            CPU(64bit)            |          |          |                    |    |
|              OpenCL              |     -     |     -     |                    |  -  |
|              Metal              |     -     |     -     |          -          |    |
|           华为麒麟 NPU           |     -     |     -     |                    |  -  |
|           华为昇腾 NPU           |          |          |          -          |  -  |
|            昆仑芯 XPU            |          |          |          -          |  -  |
|           昆仑芯 XTCL           |          |          |          -          |  -  |
|             高通 QNN             |     -     |     -     |                    |  -  |
|            寒武纪 MLU            |          |     -     |          -          |  -  |
| (瑞芯微/晶晨/恩智浦) 芯原 TIM-VX |     -     |          |                    |  -  |
|          Android NNAPI          |     -     |     -     |                    |  -  |
|            联发科 APU            |     -     |     -     |                    |  -  |
|             颖脉 NPU             |     -     |          |          -          |  -  |
|          Intel OpenVINO          |          |     -     |          -          |  -  |
|             亿智 NPU             |     -     |          |          -          |  -  |

架构设计

Paddle Lite 的架构设计着重考虑了对多硬件和平台的支持，并且强化了多个硬件在一个模型中混合执行的能力，多个层面的性能优化处理，以及对端侧应用的轻量化设计。

其中，Analysis Phase 包括了 MIR(Machine IR) 相关模块，能够对原有的模型的计算图针对具体的硬件列表进行算子融合、计算裁剪 在内的多种优化。Execution Phase 只涉及到 Kernel 的执行，且可以单独部署，以支持极致的轻量级部署。

进一步了解 Paddle Lite

如果您想要进一步了解 Paddle Lite，下面是进一步学习和使用 Paddle Lite 的相关内容：

文档和示例

* 完整文档： [Paddle Lite 文档](https://www.paddlepaddle.org.cn/lite)
* API文档：
  * [C++ API 文档](https://www.paddlepaddle.org.cn/lite/develop/api_reference/cxx_api_doc.html)
  * [Java API 文档](https://www.paddlepaddle.org.cn/lite/develop/api_reference/java_api_doc.html)
  * [Python API 文档](https://www.paddlepaddle.org.cn/lite/develop/api_reference/python_api_doc.html)
  * [CV 图像处理 API 文档](https://www.paddlepaddle.org.cn/lite/develop/api_reference/cv.html)
* Paddle Lite 工程示例： [Paddle-Lite-Demo](https://github.com/PaddlePaddle/Paddle-Lite-Demo)

关键技术

* 模型量化：
  * [静态离线量化](https://www.paddlepaddle.org.cn/lite/develop/user_guides/quant/quant_post_static.html)
  * [动态离线量化](https://www.paddlepaddle.org.cn/lite/develop/user_guides/quant/quant_post_dynamic.html)
* 调试分析：[调试和性能分析工具](https://www.paddlepaddle.org.cn/lite/develop/user_guides/profiler.html)
* 移动端模型训练：点击[了解一下](https://www.paddlepaddle.org.cn/lite/develop/demo_guides/cpp_train_demo.html)
* 飞桨预训练模型库：试试在 [PaddleHub](https://www.paddlepaddle.org.cn/hublist?filter=hot&value=1) 浏览和下载 Paddle 的预训练模型
* 飞桨推理 AI 硬件统一适配框架 NNAdapter：点击[了解一下](https://www.paddlepaddle.org.cn/lite/develop/develop_guides/nnadapter.html)

### 1.11 TensorFlow Serving/TensorFlow Lite

#### 1 TensorFlow Serving

TensorFlow Serving 是一个用于机器学习模型 serving 的高性能开源库。它可以将训练好的机器学习模型部署到线上，使用 gRPC 作为接口接受外部调用。更加让人眼前一亮的是，它支持模型热更新与自动模型版本管理。这意味着一旦部署 TensorFlow Serving 后，你再也不需要为线上服务操心，只需要关心你的线下模型训练。

TensorFlow Serving提供与 TensorFlow模型的开箱即用型集成，但也可以轻松扩展以应用其他类型的模型和数据，同时保留相同的服务器架构和API。

![TensorFlow Serving](https://img-blog.csdnimg.cn/824cec241fc742ecb6d1b708887b7056.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5q-P5aSp5LiA6KGM5Luj56CB,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

TensorFlow Serving 目前依赖 Google 的开源编译工具 [Bazel](https://github.com/bazelbuild/bazel)。Bazel 是 Google 内部编译工具 Blaze 的开源版本，功能与性能基本一致。具体的安装可以参考[官方文档](https://www.bazel.io/versions/master/docs/install.html)。此外还需要安装 [gRPC](http://www.grpc.io/) (Google 又一个内部工具的开源版)。

之后请参考[官方安装指南](https://tensorflow.github.io/serving/setup)完成。值得注意的是，最后的 bazel build 将会需要大约30分钟时间并占用约5-10G的空间（时间取决于机器性能）。配合使用 -c opt 能一定程度加快 build。

#### 2 TensorFlow Lite

TensorFlow Lite 是一款 TensorFlow 用于移动设备和嵌入式设备的轻量级解决方案。

TensorFlow 可以在多个平台上运行，从机架式服务器到小型 IoT 设备。但是随着近年来机器学习模型的广泛使用，出现了在移动和嵌入式设备上部署它们的需求。而 TensorFlow Lite 允许设备端的机器学习模型的低延迟推断。

**设计初衷**

* 轻量级：允许小 binary size 和快速初始化／启动的设备端机器学习模型进行推断。
* 跨平台：运行时的设计使其可以在不同的平台上运行，最先允许的平台是安卓和 iOS。
* 快速：专为移动设备进行优化，包括大幅提升模型加载时间，支持硬件加速。

现在，越来越多移动设备内置了专门的自定义硬件以高效处理机器学习工作负载。TensorFlow Lite 支持[安卓神经网络 API](https://developer.android.com/ndk/guides/neuralnetworks/index.html)，以充分利用新的可用加速器。

当加速器硬件不可用时，TensorFlow Lite 返回至经优化的 CPU 执行操作，确保模型仍然可在大量设备上快速运行。

**架构**

下图展示了 TensorFlow Lite 的架构设计：

![](https://static.oschina.net/uploads/space/2017/1115/164523_kZq8_2903254.png)

**组件包括**

* TensorFlow 模型（TensorFlow Model）：训练后的 TensorFlow 模型，保存在磁盘中。
* TensorFlow Lite 转换器（TensorFlow Lite Converter）：该程序将模型转换成 TensorFlow Lite 文件格式。
* TensorFlow Lite 模型文件（TensorFlow Lite Model File）：该格式基于 FlatBuffers，经过优化以适应最大速度和最小规模。

然后将 TensorFlow Lite 模型文件部署到移动 App 中：

* Java API：安卓设备上适用于 C++ API 的便利封装。
* C++ API：加载 TensorFlow Lite 模型文件，启动编译器。安卓和 iOS 设备上均有同样的库。
* 编译器（Interpreter）：使用运算符执行模型。解释器支持选择性加载运算符；没有运算符时，编译器只有 70KB，加载所有运算符后，编译器为 300KB。这比 TensorFlow Mobile（具备一整套运算符）的 1.5M 要小得多。
* 在选择的安卓设备上，编译器将使用安卓神经网络 API 进行硬件加速，或者在无可用 API 的情况下默认执行 CPU。

开发者还使用 C++ API 实现自定义 kernel，它可被解释器使用。

**模型**

TensorFlow Lite 已经支持多个面向移动端训练和优化的模型：

* MobileNet：一种能够识别超过 1000 种不同物体的视觉模型，专为移动端和嵌入式设备设计；
* Inception V3：一种图像识别模型，功能上类似于 MobileNet，但能提供更高的准确率（当然模型也更大）；
* Smart Reply：一种设备端对话模型，能对接收到的会话聊天信息提供触发性应答。第一方和第三方通信 App 可在 Android Wear 上使用该特性。

Inception v3 和 MobileNet 都在 ImageNet 数据集上训练过，你可以通过迁移学习轻松地在自己的图像数据集上重新训练这些模型。

### 1.12 libtorch

已知 以往的深度学习是以python作为编程语言；但它的深度学习训练以C++后端为基础（这些C++后端提供了 诸如 Tensor 这种class ;  CUDA优化方法 ； Auto Differentiation 等等 ）

[libtorch](https://so.csdn.net/so/search?q=libtorch&spm=1001.2101.3001.7020)就是直接以这些C++后端作为基础，使得人们可以直接用C++作为前端编程语言进行深度学习的训练。libtorch不一定比pytorch快，因为：Python 前端调用 C++ 来处理几乎任何计算成本很高的事情（尤其是任何类型的数值运算），这些运算将占用程序中花费的大部分时间。

### 1.13 ONNX(Microsoft, Facebook, IBM, Amazon)

Open Neural Network Exchange（ONNX，开放神经网络交换）格式，是一个用于表示深度学习模型的标准，可使模型在不同框架之间进行转移。ONNX是一种针对机器学习所设计的开放式的文件格式，用于存储训练好的模型。它使得不同的人工智能框架（如Pytorch, MXNet）可以采用相同格式存储模型数据并交互。ONNX的规范及代码主要由微软，亚马逊 ，Facebook 和 IBM 等公司共同开发，以开放源代码的方式托管在Github上。目前官方支持加载ONNX模型并进行推理的深度学习框架有：Caffe2, PyTorch, MXNet，ML.NET，TensorRT 和 Microsoft CNTK，并且 TensorFlow 也非官方的支持ONNX。

比方说现在某组织因为主要开发用TensorFlow为基础的框架，现在有一个深度算法，需要将其部署在移动设备上，以观测变现。传统地我们需要用caffe2重新将模型写好，然后再训练参数；试想下这将是一个多么耗时耗力的过程。

此时，ONNX便应运而生，Caffe2，PyTorch，Microsoft Cognitive Toolkit，Apache MXNet等主流框架都对ONNX有着不同程度的支持。这就便于了我们的算法及模型在不同的框架之间的迁移。无论你使用何种训练框架训练模型（比如TensorFlow/Pytorch/OneFlow/Paddle），在训练完毕后你都可以将这些框架的模型统一转换为ONNX这种统一的格式进行存储。

开放式神经网络交换（ONNX）是迈向开放式生态系统的第一步，它使AI开发人员能够随着项目的发展选择合适的工具。ONNX为AI模型提供开源格式。它定义了可扩展的计算图模型，以及内置运算符和标准数据类型的定义。最初的ONNX专注于推理（评估）所需的功能。ONNX解释计算图的可移植，它使用graph的序列化格式。它不一定是框架选择在内部使用和操作计算的形式。例如，如果在优化过程中操作更有效，则实现可以在存储器中以不同方式表示模型。

在获得ONNX模型之后，模型部署人员自然就可以将这个模型部署到兼容ONNX的运行环境中去。这里一般还会设计到额外的模型转换工作，典型的比如在Android端利用NCNN部署ONNX格式模型，那么就需要将ONNX利用NCNN的转换工具转换到NCNN所支持的bin和param格式。

ONNX作为一个文件格式，我们自然需要一定的规则去读取我们想要的信息或者是写入我们需要保存信息。ONNX使用的是Protobuf这个序列化数据结构去存储神经网络的权重信息。熟悉Caffe或者Caffe2的同学应该知道，它们的模型存储数据结构协议也是Protobuf。

Protobuf是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API（摘自官方介绍）。

Protobuf协议是一个以*.proto后缀文件为基础的，这个文件描述了用户自定义的数据结构。如果需要了解更多细节请参考0x7节的资料3，这里只是想表达ONNX是基于Protobuf来做数据存储和传输，那么自然onnx.proto就是ONNX格式文件了。

ONNX作为框架共用的一种模型交换格式，使用 protobuf 二进制格式来序列化模型，可以提供更好的传输性能我们可能会在某一任务中将 Pytorch 或者 TensorFlow 模型转化为 ONNX 模型(ONNX 模型一般用于中间部署阶段)，然后再拿转化后的 ONNX模型进而转化为我们使用不同框架部署需要的类型，**ONNX 相当于一个翻译的作用**。

![](https://pics4.baidu.com/feed/5882b2b7d0a20cf4cd20a6bbcbec003fadaf9915.jpeg@f_auto?token=131436ea2d13a4af3d07a01a427b2184)

ONNX将每一个网络的每一层或者说是每一个算子当作节点Node，再由这些Node去构建一个Graph，相当于是一个网络。最后将Graph和这个onnx模型的其他信息结合在一起，生成一个model，也就是最终的.onnx的模型。

构建一个简单的onnx模型，实质上，只要构建好每一个node，然后将它们和输入输出超参数一起塞到graph，最后转成model就可以了。

在计算方面，虽然更高级的表达不同，但不同框架产生的最终结果都是非常接近。因此实时跟踪某一个神经网络是如何在这些框架上生成的，接着使用这些信息创建一个通用的计算图，即符合ONNX标准的计算图。

ONNX为可扩展的计算图模型、内部运算器（Operator）以及标准数据类型提供了定义。在初始阶段，每个计算数据流图以节点列表的形式组织起来，构成一个非循环的图。节点有一个或多个的输入与输出。每个节点都是对一个运算器的调用。图还会包含协助记录其目的、作者等信息的元数据。运算器在图的外部实现，但那些内置的运算器可移植到不同的框架上，每个支持ONNX的框架将在匹配的数据类型上提供这些运算器的实现。

Microsoft 和合作伙伴社区创建了 ONNX 作为表示机器学习模型的开放标准。 许多框架（包括 TensorFlow、PyTorch、SciKit-Learn、Keras、Chainer、MXNet、MATLAB 和 SparkML）中的模型都可以导出或转换为标准 ONNX 格式。模型采用 ONNX 格式后，可在各种平台和设备上运行。

ONNX 运行时是一种用于将 ONNX 模型部署到生产环境的高性能推理引擎。它针对云和 Edge 进行了优化，适用于 Linux、Windows 和 Mac。它使用 C++ 编写，还包含 C、Python、C#、Java 和 Javascript (Node.js) API，可在各种环境中使用。ONNX 运行时同时支持 DNN 和传统 ML 模型，并与不同硬件上的加速器（例如，NVidia GPU 上的 TensorRT、Intel 处理器上的 OpenVINO、Windows 上的 DirectML 等）集成。通过使用 ONNX 运行时，可以从大量的生产级优化、测试和不断改进中受益。

ONNX 运行时用于大规模 Microsoft 服务，如必应、Office 和 Azure 认知服务。性能提升取决于许多因素，但这些 Microsoft 服务的 CPU 平均起来可实现 2 倍的性能提升。除了 Azure 机器学习服务外，ONNX 运行时还在支持机器学习工作负荷的其他产品中运行，包括：

l Windows:该运行时作为 Windows 机器学习的一部分内置于 Windows 中，在数亿台设备上运行。

l Azure SQL 产品系列：针对 Azure SQL Edge 和 Azure SQL 托管实例中的数据运行本机评分。

l ML.NET：在 ML.NET 中运行 ONNX 模型。

![](https://pics2.baidu.com/feed/aa18972bd40735fa6cac32875ab444ba0e24089a.jpeg@f_auto?token=0f2931f539d3ddfd5e3759f24d93aeea)

ONNX的官方网站：https://onnx.ai/

ONXX的GitHub地址：https://github.com/onnx/onnx

## 2 训练编译优化

### 2.1 XLA(谷歌)

XLA的全称是Accelerated Linear Algebra，即加速线性代数。作为一种深度学习编译器，长期以来被作为Tensorflow框架的一个试验特性被开发，历时至今已经超过两三年了，随着Tensorflow 2.X的发布，XLA也终于从试验特性变成了默认打开的特性。此外， Pytorch社区也在大力推动XLA在Pytorch下的开发，现在已经有推出PyTorch/XLA TPU版本，暂只支持谷歌平台TPU上使用。

[神经网络编译加速之XLA简介]()

[XLA优化原理简介](https://developer.huawei.com/consumer/cn/forum/topic/0201750315901780148)

### 2.2 Faster Transformer

Faster Transformer是一个开源的高效Transformer实现，相比TensorFlow XLA 可以带来1.5-2x的提速。Faster Transformer对外提供C++ API， TensorFlow OP，以及TensorRT Plugin三种接口。对每种接口的调用方式，我们提供了完整的示例，方便用户集成。

[NVIDIA BERT推理解决方案Faster Transformer开源啦](https://mp.weixin.qq.com/s/77mh--Z2dUbz6sTncNZIYA)

[Faster Transformer](https://github.com/NVIDIA/FasterTransformer)

## 3 python库

### 3.1 deepspeed（分布式，显存优化，量化，MoE，分布式优化器）

### 3.2 Transformers Accelerate（数据并行分布式训练，混合精度）

### 3.3 Pytorch-Lightning Fabric(分布式训练)

### 3.4 Apex（混合精度训练，已集成到pytorch）

### 3.3 Pytorch Quantization（ptorch量化包）

### 3.4 GPipe(流水线并行)


## 4 编译器

### 1. MLIR

#### **1.1. 简介**

MLIR是LLVM原作者Chris Lattner在Google时候开始做的项目，现在已经合入LLVM仓库。MLIR即Multi-Level Intermediate Representation，多级的中间表示。MLIR目的是做一个通用、可复用的 **编译器框架** ，减少构建Domain Specific Compiler的开销。MLIR目前主要用于机器学习领域，但设计上是通用的编译器框架，比如也有FLANG（llvm中的FORTRAN编译器），CIRCT（用于硬件设计）等与ML无关的项目。MLIR现在还是早期阶段，还在快速更新迭代，发展趋势是尽可能完善功能，减少新增自定义feature的工作量。

#### **1.2. 架构**

IR部分（包括builtin dialect）构成了MLIR的核心，这部分接口相对比较稳定。IR里面有Dialect，Operation，Attribute，Type，Context等组成部分。自带的一些Dialect（如std，scf，linalg等）是类似标准库的东西，把一些通用的东西抽象出来，增加复用性，减少开发者的工作量。开发者可以选用自己想要的dialect，而且不同的dialect之间支持混合编程。这样的设计保证了自由度和灵活性。Transforms一般是Dialect内的变换，Conversion是Dialect间的变换，Translation是MLIR与非MLIR（比如LLVM IR，C++ source code, SPIRV）之间的变换。

![](https://pic1.zhimg.com/v2-b752db1a3c5c0e53314703f8e92b88e0_r.jpg)

#### **1.3. 与LLVM区别**

个人认为MLIR更适合和LLVM做比较，而不是TVM等dl compiler。LLVM和MLIR的很多概念都比较像，了解LLVM的话MLIR会比较容易上手。

LLVM IR由于当时的历史局限性，类型只设计了标量和定长vector，有个给LLVM加matrix类型的提案目前看来也没有进展。而MLIR自带tensor类型，对深度学习领域更友好。

MLIR有Operation和Dialect的概念，Dialect，Operation，Attribute，Type等都可以通过td文件比较方便地定义出来。而LLVM定义新的intrinsic比较麻烦，定义新的IR就更麻烦了。LLVM IR主要表示硬件指令操作，而MLIR能表示更多东西，比如表示神经网络的图结构。因为有Dialect，MLIR是组件化，去中心的，不像LLVM的ir是一种大而全的。

MLIR执行过程和LLVM一样，IR会过由Pass组成的Pipeline，不断地变换生成最终的IR。不同的是MLIR的IR可以是不同dialect的，构成了Multi-Level的效果。

#### **1.4. MLIR开源项目**

mlir只是个编译器框架，本身并没有什么完整功能。所以可以参考一些基于MLIR做的开源项目。

* tensorflow：没有tf就没有MLIR
* mhlo：tensorflow组件，相当于支持动态规模的XLA
* tfrt：tensorflow组件，tf新的runtime
* torch-mlir：连接pytorch与mlir生态
* onnx-mlir：连接onnx与mlir生态
* iree：深度学习end2end编译器
* circt：硬件设计及软硬件协同开发
* flang：FORTRAN的编译器前端
* polygeist：C/C++ source code变成mlir Affine

### 2. LLVM

> LLVM项目是模块化、可重用的编译器以及工具链技术的集合。

美国计算机协会 (ACM) 将其2012 年软件系统奖项颁给了LLVM，之前曾经获得此奖项的软件和技术包括:Java、Apache、 Mosaic、the World Wide Web、Smalltalk、UNIX、Eclipse等等
创始人:Chris Lattner，亦是Swift之父

> 趣闻：Chris Latter本来只是想写一个底层的虚拟机，这也是LLVM名字的由来，low level virtual machine，跟Java的JVM虚拟机一样，可是后来，llvm从来没有被用作过虚拟机，哪怕LLVM的名气已经传开了。所以人们决定仍然叫他LLVM，更多的时候只是当作“商标”一样的感觉在使用，其实它跟虚拟机没有半毛钱关系。官方描述如下
> The name "LLVM" itself is not an acronym; it is the full name of the project. “LLVM”这个名称本身不是首字母缩略词; 它是项目的全名。

#### 2.1传统的编译器架构

传统编译器架构

* **Frontend:前端**
  词法分析、语法分析、语义分析、生成中间代码
* **Optimizer:优化器**
  中间代码优化
* **Backend:后端**
  生成机器码

#### 2.2 LLVM架构

* 不同的前端后端使用统一的中间代码LLVM Intermediate Representation (LLVM IR)
* 如果需要支持一种新的编程语言，那么只需要实现一个新的前端
* 如果需要支持一种新的硬件设备，那么只需要实现一个新的后端
* 优化阶段是一个通用的阶段，它针对的是统一的LLVM IR，不论是支持新的编程语言，还是支持新的硬件设备，都不需要对优化阶段做修改
* 相比之下，GCC的前端和后端没分得太开，前端后端耦合在了一起。所以GCC为了支持一门新的语言，或者为了支持一个新的目标平台，就 变得特别困难
* LLVM现在被作为实现各种静态和运行时编译语言的通用基础结构(GCC家族、Java、.NET、Python、Ruby、Scheme、Haskell、D等)

#### 2.3 什么是[Clang](https://links.jianshu.com/go?to=http%3A%2F%2Fclang.%2F)

LLVM项目的一个子项目，基于LLVM架构的C/C++/Objective-C编译器前端。

***相比于GCC，Clang具有如下优点***

* 编译速度快:在某些平台上，Clang的编译速度显著的快过GCC(Debug模式下编译OC速度比GGC快3倍)
* 占用内存小:Clang生成的AST所占用的内存是GCC的五分之一左右
* 模块化设计:Clang采用基于库的模块化设计，易于 IDE 集成及其他用途的重用
* 诊断信息可读性强:在编译过程中，Clang 创建并保留了大量详细的元数据 (metadata)，有利于调试和错误报告
* 设计清晰简单，容易理解，易于扩展增强

#### 2.4 Clang与LLVM关系

Clang与LLVM

LLVM整体架构，前端用的是clang，广义的LLVM是指整个LLVM架构，一般狭义的LLVM指的是LLVM后端（包含代码优化和目标代码生成）。

源代码（c/c++）经过clang--> 中间代码(经过一系列的优化，优化用的是Pass) --> 机器码
