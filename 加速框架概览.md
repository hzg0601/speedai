# 加速框架概览


[开源AI框架学习路线](https://zhuanlan.zhihu.com/p/467904500)

![](https://pic3.zhimg.com/v2-b6774b41dad71e96639d87dc036d600e_r.jpg)

整体上，左侧采用以生产流程环节的不同方式来呈现。由上至下依次为：数据准备阶段、（分布式）训练阶段、编译优化阶段、推理生产阶段。最下层为硬件基础。而右侧上部分为提高研发效能的辅助系统。下半部分是分布式基础设施。

## 1训练加速

## 2 推理加速

### 2.1 加速推理器TensorRT

TensorRT是[nvidia](https://so.csdn.net/so/search?q=nvidia&spm=1001.2101.3001.7020)家的一款高性能深度学习推理SDK。此SDK包含深度学习推理优化器和运行环境，可为深度学习推理应用提供低延迟和高吞吐量。在推理过程中，基于TensorRT的应用程序比仅仅使用CPU作为平台的应用程序要快40倍。

[TensorRT入门](https://zhuanlan.zhihu.com/p/371239130)

[TensorRT介绍](https://blog.csdn.net/weixin_42111770/article/details/114336102)

### 2.2 推理服务器Triton

推理识别是人工智能最重要的落地应用，其他与深度学习相关的数据收集、标注、模型训练等工作，都是为了得到更好的最终推理性能与效果。

几乎每一种深度学习框架都能执行个别的推理工作，包括 Tensorflow、Pytorch、MXNet 等通用型框架与 YOLO 专属的 Darknet 框架，此外还有 ONNX 开发推理平台、NVIDIA TensorRT 加速推理引擎，也提供推理相关的 C / C++ 与 Python 开发接口，这是大部分技术人员所熟悉的方法。

在垂直应用方面，NVIDIA 的 **DeepStream** 智能分析工具是非常适合用在种类固定且需要长期统计分析的场景，包括各种交通场景的人 / 车流量分析、工业流水线质量检测等应用，并且在早期视觉（Visualization）类推理功能之上，再添加对话（Conversation）类推理功能，让使用范围更加完整。

上述的推理方式通常适合在识别固定种类与固定输入源的使用场景，在交通、工业自动化领域、无人设备等领域的使用比较普及。

**但是这种方式并不适合在网络相关的服务类应用中使用** ，包括在线的产品推荐、图像分类、聊天机器人等应用， **因为在线服务需要同时面对未知数量与类型的数据源，并且透过 HTTP 协议进行数据传输的延迟问题** ，也是严重影响用户体验感的因素，这是绝大部分网路服务供应商要导入 AI 智能识别技术所面临的共同难题。

**NVIDIA Triton** 推理服务器的最大价值，便是 **为服务类智能应用提供一个完整的解决方案** ，因此首先需要解决以下的三大关键问题：

**1. 高通用性：**

(1) 广泛支持多种计算处理器：包括具备 NVIDIA GPU 的 x86 与 ARM CPU 设备，也支持纯 CPU 设备的推理计算。

(2) 广泛支持各种训练框架的文件格式：包括 TensorFlow 1.x/2.x、PyTorch、ONNX、TensorRT、RAPIDS FIL（用于 XGBoost、Scikit-learn Random Forest、LightGBM）、OpenVINO、Python 等。

(3) 广泛支持各种模型种类：包括卷积神经网络 (CNN)、循环神经网络 (RNN)、决策树、随机森林和图神经网络等算法。

**2.部署便利：**

(1) 可在横向扩展的云或数据中心、企业边缘，甚至 NVIDIA Jetson 等嵌入式设备上运行。

(2) 支持用于 AI 推理的裸机和虚拟化环境，包括 VMware vSphere 与基于 Docker 技术的 Kubernetes 管理机制。

(3) 可托管于多种人工智能云平台，包括 Amazon SageMaker、Azure ML、Google Vertex AI、阿里巴巴 AI、腾讯 TI-EMS 等平台。

**3.性能优化：**

(1) **动态批量处理** ：推理优化的一个因素是批量大小，或者您一次处理多少个样本，GPU 以更高的批量提供高吞吐量。然而，对于实时应用程序，服务的真正限制不是批量大小甚至吞吐量，而是为最终客户提供出色体验所需的延迟。

(2) **模型并发执行** ：GPU 是能够同时执行多个工作负载的计算设备，NVIDIA Triton 推理服务器通过在 GPU 上同时运行多个模型来最大限度地提高性能并减少端到端延迟，这些模型可以是相同的，也可以是来自不同框架的不同模型。GPU 内存大小是同时运行模型数量的唯一限制，这会影响GPU利用率和吞吐量。

[NVIDIA Triton系列文章（1）：应用概论](https://zhuanlan.zhihu.com/p/577131256)

https://developer.nvidia.cn/zh-cn/nvidia-triton-inference-server

https://github.com/triton-inference-server/server

## 3 编译器

[Faster Transformer](https://github.com/NVIDIA/FasterTransformer)

Faster Transformer是一个开源的高效Transformer实现，相比TensorFlow XLA 可以带来1.5-2x的提速。Faster Transformer对外提供C++ API， TensorFlow OP，以及TensorRT Plugin三种接口。对每种接口的调用方式，我们提供了完整的示例，方便用户集成。

[NVIDIA BERT推理解决方案Faster Transformer开源啦](https://mp.weixin.qq.com/s/77mh--Z2dUbz6sTncNZIYA)

[XLA优化原理简介](https://developer.huawei.com/consumer/cn/forum/topic/0201750315901780148)

XLA的全称是Accelerated Linear Algebra，即加速线性代数。作为一种深度学习编译器，长期以来被作为Tensorflow框架的一个试验特性被开发，历时至今已经超过两三年了，随着Tensorflow 2.X的发布，XLA也终于从试验特性变成了默认打开的特性。此外， Pytorch社区也在大力推动XLA在Pytorch下的开发，现在已经有推出PyTorch/XLA TPU版本，暂只支持谷歌平台TPU上使用。

[神经网络编译加速之XLA简介]()
