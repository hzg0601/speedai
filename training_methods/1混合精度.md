# 1混合精度训练

## 1.1 基本原理

那为什么不单独使用Float32或Float16，而是两种类型混合呢？原因是：在某些情况下Float32有优势，而在另外一些情况下Float16有优势。这里先介绍下FP16：

　　优势有三个：

　１．减少显存占用；

　２．加快训练和推断的计算，能带来多一倍速的体验；

　３．张量核心的普及（NVIDIA　Tensor Core）,低精度计算是未来深度学习的一个重要趋势。

　　但凡事都有两面性，FP16也带来了些问题：１．溢出错误；２．舍入误差；

１．溢出错误：由于FP16的动态范围比FP32位的狭窄很多，因此，在计算过程中很容易出现上溢出和下溢出，溢出之后就会出现"NaN"的问题。在深度学习中，由于激活函数的梯度往往要比权重梯度小，更易出现下溢出的情况

![](https://pic1.zhimg.com/v2-8eb54bd19b0878947f6b8d28806fb440_r.jpg)

2.舍入误差

　舍入误差指的是当梯度过小时，小于当前区间内的最小间隔时，该次梯度更新可能会失败：

![](https://pic3.zhimg.com/v2-933624420668b99795bd1e7e48739f5a_r.jpg)

　为了消除torch.HalfTensor也就是FP16的问题，需要使用以下两种方法：

**解决方案**

**损失缩放 (Loss Scaling)**

为了解决下溢出的问题，论文中对计算出来的 loss 值进行缩放 (scale)，由于链式法则的存在，对 loss 的缩放会作用在每个梯度上。缩放后的梯度，就会平移到 FP16 的有效范围内。这样就可以用 FP16 存储梯度而又不会溢出了。此外，在进行更新之前，需要**先将缩放后的梯度转化为 FP32，再将梯度反缩放 (unscale) 回去。 **

注意这里一定要先转成 FP32，不然 unscale 的时候还是会下溢出。

缩放因子 (loss_scale) 一般都是框架自动确定的，只要没有发生 inf 或者 nan，loss_scale 越大越好。因为随着训练的进行，网络的梯度会越来越小，更大的 loss_scale 可以更加充分地利用 FP16 的表示范围。

**FP32 权重备份**

为了实现 FP16 的训练，我们需要把模型权重和输入数据都转成 FP16，反向传播的时候就会得到 FP16 的梯度。如果此时直接进行更新，因为**梯度 * 学习率**的值往往较小，和模型权重的差距会很大，可能会出现舍入误差的问题。

所以解决思路是：将**模型权重、激活值、梯度**等数据用 **FP16 来存储，同时维护一份 FP32** 的模型权重副本用于更新。在反向传播得到 FP16 的梯度以后， **将其转化成 FP32 并 unscale** ，最后更新 FP32 的模型权重。因为整个更新过程是在 FP32 的环境中进行的，所以不会出现舍入误差。

**FP32 权重备份解决了反向传播的舍入误差问题。**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmibBs7A4drwMgTrKeicjyABOnbfkbCp4GZibcPKrbsOnlBaIg65ADLt1uwciahuLNrITtqYYiaOxiaXySw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1 "图片")

**黑名单**

对于那些在 FP16 环境中运行不稳定的模块，我们会将其添加到黑名单中，强制它在 FP32 的精度下运行。比如需要计算 batch 均值的 BN 层就应该在 FP32 下运行，否则会发生舍入误差。还有一些函数对于算法精度要求很高，比如 torch.acos()，也应该在 FP32 下运行。论文中的黑名单只包含 BN 层。

如何保证黑名单模块在 FP32 环境中运行：以 BN 层为例，将其权重转为 FP32，并且将输入从 FP16 转成 FP32，这样就可以保证整个模块是在 FP32 下运行的。

**黑名单解决了某些函数在 FP16 环境下的算术不稳定的问题。**

**Tensor Core**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmibBs7A4drwMgTrKeicjyABOu3gsczSVq7SmicgGlWS1YicygZZ73OgWr5Cb1tU1MCibZQhm1POia47nzA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1 "图片")

Tensor Core 可以让 FP16 做矩阵相乘，然后把结果累加到 FP32 的矩阵中。这样既可以享受 FP16 高速的矩阵乘法，又可以利用 FP32 来消除舍入误差。

## 1.2 BFloat16和TensorFloat32格式

```
import torch
# Creates once at the beginning of training
scaler = torch.cuda.amp.GradScaler()

for data, label in data_iter:
   optimizer.zero_grad()
   # Casts operations to mixed precision
   with torch.amp.autocast(device_type=“cuda”, dtype=torch.bfloat16):
      loss = model(data)

   # Scales the loss, and calls backward()
   # to create scaled gradients
   scaler.scale(loss).backward()

   # Unscales gradients and calls
   # or skips optimizer.step()
   scaler.step(optimizer)

   # Updates the scale for next iteration
   scaler.update()
```

[https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/)

## 1.3 工具和库

apex 的源码中 FP16 的梯度会先转化为 FP32，再做更新，所以权重更新和 Tensor Core 并无关系。

o0 是纯 FP32，用来当精度的基准。o3 是纯 FP16，用来当速度的基准。

重点讲 o1 和 o2 。我们之前讲的 AMP 策略其实就是 o2: 除了 BN 层的权重和输入使用 FP32，模型的其余权重和输入都会转化为 FP16。此外还会创建一个 FP32 的权重副本来执行更新操作。

和 o2 不同， o1 不再需要 FP32 权重备份，因为 o1 的模型一直都是 FP32。可能有些读者会好奇，既然模型参数是 FP32，那怎么在训练过程中使用 FP16 呢？答案是 o1 建立了一个 PyTorch 函数的黑白名单，对于白名单上的函数，强制要求其用 FP16，即会将函数的参数先转化为 FP16，再执行函数本身。黑名单则强制要求 FP32。

o1 还有一个细节: 虽然白名单上的 PyTorch 函数是以 FP16 运行的，但是产生的梯度是 FP32，所以不需要手动将其转成 FP32 再 unscale，直接 unscale 即可。

如果说 o1是 FP16 + FP32，更加激进的 o2 就是 almost FP16 (几乎全是 FP16)。通常来说 o1 比 o2 更稳，一般先选择 o1，再尝试 o2 看是否掉点，如果不掉点就用 o2。

[PyTorch的自动混合精度（AMP）](https://zhuanlan.zhihu.com/p/408610877)

[由浅入深的混合精度训练教程](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247550159&idx=5&sn=f5db2afa547970bc429112e32d2e7daf&chksm=ebb73c1bdcc0b50d0e85039bd5d8349a23330e3e0f138a7dd2da218a20174d0965837682dd14&scene=27)
