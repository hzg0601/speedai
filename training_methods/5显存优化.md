# 5 显存优化

## 5.1 ZeRO的显存优化

## 5.2 ZeRO-Offload和ZeRO-Infinity

拿通信换显存的一种方法，简单来说就是让模型参数、激活值等在CPU内存和GPU显存之间左右横跳。

* ZeRO-Offload: Democratizing Billion-Scale Model Training
* ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning

## 5.3 Gradient Checkpoint


梯度检查点的工作原理是 **在反向传播时重新计算深度神经网络的中间值（而通常情况是在前向传播时存储的)** 。这个策略是用时间（重新计算这些值两次的时间成本）来换空间（提前存储这些值的内存成本）。

神经网络使用的总内存基本上是两个部分的总和。

* 第一部分是模型使用的 **静态内存** 。尽管 PyTorch 模型中内置了一些固定开销，但总的来说几乎完全由**模型权重**决定。而如今，在生产中使用的现代深度学习模型的总参数在100万到10亿之间。作为参考，一个带 16GB GPU 内存的 NVIDIA T4 的实际限制大约在1-1.5亿个参数之间。
* 第二部分是模型的计算图所占用的 **动态内存** 。在训练模式下，每次通过神经网络的**前向传播**都为网络中的每个神经元计算一个 **激活值** ，这个值随后**被存储在所谓的计算图**中。必须为批次中的每个单个训练样本存储一个值，因此数量会迅速的累积起来。 **总成本取决于模型大小和批处理大小** ，并设置适用于您的GPU内存的最大批处理大小的限制。一开始存储激活的原因是， **在反向传播期间计算梯度时需要用到激活** 。

大型模型在静态和动态方面都很耗资源。首先，它们很难适配 GPU，而且哪怕你把它们放到了设备上，也很难训练，因为批处理大小被迫限制的太小而无法收敛。

**梯度检查点（gradient checkpointing）的工作原理是从计算图中**省略一些激活值（由前向传播产生，其中这里的”一些“是指可以只省略模型中的部分激活值，折中时间和空间，陈天奇在它的论文中[Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174) **使用了如下动图的方法，即前向传播的时候存一个节点释放一个节点，空的那个等需要用的时候再backword的时候重新计算）** 。这减少了计算图使用的内存，降低了总体内存压力（并允许在处理过程中使用更大的批次大小）。

![动图封面](https://pic3.zhimg.com/v2-1679b74a85687cdb250e532931bb266a_b.jpg "动图封面")

PyTorch 通过 `torch.utils.checkpoint.checkpoint`和 `torch.utils.checkpoint.checkpoint_sequential`提供梯度检查点，根据官方文档的 notes，它实现了以下功能， **在前向传播时，PyTorch 将保存模型中的每个函数的输入元组** 。在反向传播过程中，对于每个函数，输入元组和函数的组合以 **实时的方式重新计算** ， **插入到每个需要它的函数的梯度公式中** ， **然后丢弃（显存中只保存输入数据和函数）** 。网络计算开销大致相当于每个样本通过模型前向传播开销的两倍。

梯度检查点首次发表在2016年的论文**[《Training Deep Nets With Sublinear Memory Cost》](https://arxiv.org/pdf/1604.06174.pdf)**中。论文声称提出的梯度检查点算法将模型的动态内存开销从 O(n) （`n`为模型中的层数）降低到 O(n) ，并通过实验展示了将 ImageNet 的一个变种从 48GB 压缩到了 7GB 内存占用。

重计算将前向计算的激活值丢弃，在后向计算时再重新进行计算，节省了巨量的激活值开销。

pytorch的话用torch.utils.checkpoint就可以实现，很方便。

原始论文：Algorithm 799: Revolve: An implementation of checkpointing for the reverse or adjoint mode of computational differentiation

陈天奇最早把他带到了机器学习里：Training Deep Nets with Sublinear Memory Cost

## 5.4 Optimizer

### 5.4.1 分布式Optimizer

### 5.4.2 显存友好的Optimizer

### 5.4.3 低精度存储Optimizer状态的Optimizer
