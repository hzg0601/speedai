# 5 显存优化

## 5.1 ZeRO的显存优化（ZeRO-R）

ZeRO-R用于减少residual states所占的冗余内存。ZeRO-R对residual states的三个方面都做了内存优化：

1) 对于激活函数值，一般使用checkpoint来优化存储(关于checkpoint的概念见[打工伟：谷歌机器学习加速库Gpipe](https://zhuanlan.zhihu.com/p/462083026))，但是这对于大模型仍旧不足。因此，ZeRO-R通过激活函数分区来移除一些冗余的激活函数存储，当某个GPU需要使用到某个激活函数且该激活函数值并未存储在该GPU时，则通过all-gather操作在GPU间对所需的激活函数值进行通信获取。此外，ZeRO-R在适当的时候会将激活函数移动至CPU内存存储，即CPU-offload。
2) ZeRO-R为临时缓冲区定义了适当的大小，从而实现内存和计算效率的平衡。ZeRO-R使用恒定大小的缓冲区(之前的缓冲区大小通常根据随着模型大小的变化而变化)，以避免临时缓冲区随着模型大小的增加而爆炸，同时使它们足够大以保持效率。
3) ZeRO-R根据张量的不同生命周期主动管理内存，防止内存碎片。模型训练中的内存碎片是由激活函数的checkpoint和梯度计算的结果：

* 除了激活函数的checkpoint以外，剩下的所需的激活函数在反向传播过程中都需要重新计算，这便出现了短生命周期内存（需要重计算的激活函数）和长生命周期内存（激活函数checkpoint）的交叉，从而出现内存碎片。
* 类似地，在反向传播期间，模型参数梯度所占内存为长生命周期的，而激活函数的梯度所占内存为短生命周期的，它们之间的交叉也导致了内存碎片。

ZeRO-R通过为激活函数checkpoint和模型参数梯度预先分配至连续的内存缓冲区中，这不仅增加了内存可用性，而且还通过减少内存分配器查找空闲连续内存的时间来提高效率。

## 5.2 ZeRO-Offload和ZeRO-Infinity

拿通信换显存的一种方法，简单来说就是让模型参数、激活值等在CPU内存和GPU显存之间左右横跳。

Offload 技术

ZeRO-Offload [10] 技术主要思想是将部分训练阶段的模型状态 offload 到内存，让 CPU 参与部分计算任务。为了避免 GPU 和 CPU 之间的通信开销，以及 CPU 本身计算效率低于 GPU 这两个问题的影响。Offload 的作者在分析了 adam 优化器在 fp16 模式下的运算流程后，考虑只将模型更新的部分下放至 CPU 计算，即让 CPU 充当 Parameter Server 的角色。如下图所示：

![d46f60b16747e7394c52903cfc0aa053.png](https://img-blog.csdnimg.cn/img_convert/d46f60b16747e7394c52903cfc0aa053.png)

同时为了提高效率，Offload 的作者提出可以将通信和计算的过程并行起来，以降低通信对整个计算流程的影响。具体来说，GPU 在反向传播阶段，可以待梯度值填满bucket后，一边计算新的梯度一边将bucket传输给CPU；当反向传播结束，CPU基本上获取了最新的梯度值。同样的，CPU在参数更新时也同步将已经计算好的参数传给GPU，如下图所示：

![e4ee8957994272eef5fdac034f2408fa.png](https://img-blog.csdnimg.cn/img_convert/e4ee8957994272eef5fdac034f2408fa.png)

最后作者也分析了多卡的情况，证明了他提出的方案具有可扩展性。

ZeRO-Infinity: 利用NVMe打破GPU显存墙。[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/pdf/2104.07857.pdf) 发表在SC 21，同样是进行offload，ZeRO-Offload更侧重单卡场景，而ZeRO-Infinity则是典型的工业界风格，奔着极大规模训练去了。

* ZeRO-Offload: Democratizing Billion-Scale Model Training
* ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning
* [DeepSpeed之ZeRO系列：将显存优化进行到底](https://zhuanlan.zhihu.com/p/513571706)

## 5.3 Gradient Checkpoint

梯度检查点的工作原理是 **在反向传播时重新计算深度神经网络的中间值（而通常情况是在前向传播时存储的)** 。这个策略是用时间（重新计算这些值两次的时间成本）来换空间（提前存储这些值的内存成本）。

神经网络使用的总内存基本上是两个部分的总和。

* 第一部分是模型使用的 **静态内存** 。尽管 PyTorch 模型中内置了一些固定开销，但总的来说几乎完全由**模型权重**决定。而如今，在生产中使用的现代深度学习模型的总参数在100万到10亿之间。作为参考，一个带 16GB GPU 内存的 NVIDIA T4 的实际限制大约在1-1.5亿个参数之间。
* 第二部分是模型的计算图所占用的 **动态内存** 。在训练模式下，每次通过神经网络的**前向传播**都为网络中的每个神经元计算一个 **激活值** ，这个值随后**被存储在所谓的计算图**中。必须为批次中的每个单个训练样本存储一个值，因此数量会迅速的累积起来。 **总成本取决于模型大小和批处理大小** ，并设置适用于您的GPU内存的最大批处理大小的限制。一开始存储激活的原因是， **在反向传播期间计算梯度时需要用到激活** 。

大型模型在静态和动态方面都很耗资源。首先，它们很难适配 GPU，而且哪怕你把它们放到了设备上，也很难训练，因为批处理大小被迫限制的太小而无法收敛。

**梯度检查点（gradient checkpointing）的工作原理是从计算图中**省略一些激活值（由前向传播产生，其中这里的”一些“是指可以只省略模型中的部分激活值，折中时间和空间，陈天奇在它的论文中[Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174) **使用了如下动图的方法，即前向传播的时候存一个节点释放一个节点，空的那个等需要用的时候再backword的时候重新计算）** 。这减少了计算图使用的内存，降低了总体内存压力（并允许在处理过程中使用更大的批次大小）。

![动图封面](https://pic3.zhimg.com/v2-1679b74a85687cdb250e532931bb266a_b.jpg "动图封面")

PyTorch 通过 `torch.utils.checkpoint.checkpoint`和 `torch.utils.checkpoint.checkpoint_sequential`提供梯度检查点，根据官方文档的 notes，它实现了以下功能， **在前向传播时，PyTorch 将保存模型中的每个函数的输入元组** 。在反向传播过程中，对于每个函数，输入元组和函数的组合以 **实时的方式重新计算** ， **插入到每个需要它的函数的梯度公式中** ， **然后丢弃（显存中只保存输入数据和函数）** 。网络计算开销大致相当于每个样本通过模型前向传播开销的两倍。

梯度检查点首次发表在2016年的论文**[《Training Deep Nets With Sublinear Memory Cost》](https://arxiv.org/pdf/1604.06174.pdf)**中。论文声称提出的梯度检查点算法将模型的动态内存开销从 O(n) （`n`为模型中的层数）降低到 O(n) ，并通过实验展示了将 ImageNet 的一个变种从 48GB 压缩到了 7GB 内存占用。

重计算将前向计算的激活值丢弃，在后向计算时再重新进行计算，节省了巨量的激活值开销。

pytorch的话用torch.utils.checkpoint就可以实现，很方便。

原始论文： An implementation of checkpointing for the reverse or adjoint mode of computational differentiation

陈天奇最早把他带到了机器学习里：Training Deep Nets with Sublinear Memory Cost

## 5.4 Optimizer

### 5.4.1 分布式Optimizer

在目前模型训练的过程中，直接使用大批量的训练方式可能导致模型训练不稳定。最早有 Facebook 的研究 [16] 表明，通过线性调整学习率，并配合 warmup 等辅助手段，让学习率随 batch 的增大而线性增大，即可在ResNet-50上将 batch size 增大至 8K 时仍不影响模型性能。但该方法在 AlexNet 等网络失效，在 LARS [17] 优化器这篇论文中，作者尤洋在实验中发现不同层的权值和其梯度的 2 范数的比值差异很大，据此基于带动量的SGD优化器提出LARS优化器。核心算法如下图所示：

![5a67695d34f76a8c86e7d28fa6f7fffe.png](https://img-blog.csdnimg.cn/img_convert/5a67695d34f76a8c86e7d28fa6f7fffe.png)

基于以上的思路，尤洋将上述方法扩展到Adam优化器，提出了LAMB [18] 优化器：

![8effd1f70154a20a6bade2046ea26ff6.png](https://img-blog.csdnimg.cn/img_convert/8effd1f70154a20a6bade2046ea26ff6.png)

### 5.4.2 显存友好的Optimizer

比较早期的工作是如 Adafactor [12] 主要是针对 Adam 进行优化的，它取消了 Adam 中的动量项，并使用矩阵分解方法将动量方差项分解成两个低阶矩阵相乘来近似实现 Adam 的自适应学习率功能。

### 5.4.3 低精度存储Optimizer状态的Optimizer

使用低精度量化方式存储优化器状态的优化器，如 8 bit Optimizer [13]，核心思想是将优化器状态量化至 8 bit 的空间，并通过动态的浮点数表示来降低量化的误差。还有更加激进的使用 1 bit 量化优化器的方法，如 1-bit Adam [14] 和 1-bit LAMB [15]。他们主要是使用压缩补偿方法的来减少低精度量化对模型训练的影响。

### 5.5 算子融合

算子融合实际上是将若干个 CUDA 上的运算合成一个运算，本质上是减少了 CUDA 上的显存读写次数。举个例子，对于一个线性层 + batch norm + activation 这个组合操作来说：

![e4db53a5e3f317fffc6c098fe6df3fb6.png](https://img-blog.csdnimg.cn/img_convert/e4db53a5e3f317fffc6c098fe6df3fb6.png)

直接使用 PyTorch 实现的会在计算**y** **~1~** ,  ****y****~2~**** , ****y****~3~****的过程中分别产生一次显存的读和写操作，即3次读和写。如果将其按下面的公式合并成一个算子进行计算，那么中间的结果可以保留在 GPU 上的寄存器或缓存中，从而将显存读写次数降低至1次。

![dd0133ce447e2df77cd5c588c7035d75.png](https://img-blog.csdnimg.cn/img_convert/dd0133ce447e2df77cd5c588c7035d75.png)

目前 PyTorch 可以使用 torch.jit.script 来将函数或 nn.Module 转化成 TorchScript 代码，从而实现算子融合。

> https://pytorch.org/docs/stable/generated/torch.jit.script.html

参考文献

```go
[1] PyTorch Distributed: Experiences on Accelerating Data Parallel Training
[2] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
[3] An Efficient 2D Method for Training Super-Large Deep Learning Models
[4] 2.5-dimensional distributed model training
[5] Maximizing Parallelism in Distributed Training for Huge Neural Networks
[6] Sequence Parallelism: Long Sequence Training from System Perspective
[7] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
[8] PipeDream: Fast and Efficient Pipeline Parallel DNN Training
[9] ZeRO: memory optimizations toward training trillion parameter models
[10] ZeRO-Offload: Democratizing Billion-Scale Model Training
[11] Training Deep Nets with Sublinear Memory Cost
[12] Adafactor: Adaptive Learning Rates with Sublinear Memory Cost
[13] 8-bit Optimizers via Block-wise Quantization
[14] 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed
[15] 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed
[16] Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour
[17] Large Batch Training of Convolutional Networks
[18] Large Batch Optimization for Deep Learning: Training BERT in 76 minutes
[19] Mixed Precision Training
[20] Bringing HPC Techniques to Deep Learning
[21] A Survey of Transformers
```

![](https://img-blog.csdnimg.cn/img_convert/c944df8bb8829655cf4325e9da043fc9.png)
