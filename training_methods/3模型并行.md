# 2 模型并行

模型并行的话可以分为两大类：分别是层间模型并行(inter-layer)和层内模型并行(intra-layer)。NUS的文章基本上都是针对intra-layer的model-paralleism，或者也可以叫tensor parallelism。

层内模型并行可以对输入矩阵，或者参数矩阵进行其中一个维度的切分，并将切片放到不同的设备上去。典型例子就是1D的Megatron。

层间模型并行则是对模型层进行切分，业界也有很多做框架的公司管它叫Pipeline并行，但是我的观点是层间模型并行只有真的流水起来了才能够叫Pipeline并行。

假设我们现在有两层模型，两张卡，对模型分别进行层间模型并行，按W的input_size进行切分的层内模型并行，以及按W的output_size进行切分的层内模型并行。则有下表：

|          | 层间                         | 1D层内(按in_size切分)     | 1D层内(按out_size切分)                                   |
| -------- | ---------------------------- | ------------------------- | -------------------------------------------------------- |
| 计算时间 | 串行执行，较慢               | 并行执行，更快            | 并行执行，更快                                           |
| 通信量   | send/recv 第一层的中间值大小 | 两层的输出均需做Allreduce | 两层的输出需做Allgather``后向传播中输入误差需要Allreduce |
| 内存开销 | 总开销与单机单卡情况一致     | 激活值增加（输出）        | 激活值增加（输入）                                       |

## 2.1 2D模型并行范式Optimus

简单来说，2D模型并行范式Optimus采用了SUMMA 算法，也就是可扩展通用矩阵乘法算法(Scalable universal matrix multiplication algorithm)。该方法将矩阵平均地切分成了 q×q 个子块，每个子块对应的放到一个设备上。这种网格状的划分对于谷歌的TPU来说可太香了。

SUMMA将 C=AB 这样的矩阵乘法转化成了点积的和。如下图所示，其中矩阵 A=[A1,A2,...,Ap];B=[B1,B2,...,Bp]T 。

![](https://pic2.zhimg.com/v2-883208e10d5f06061ebe4f995c67fbad_r.jpg)

SUMMA: C=AB

反向的过程我就不说了，大同小异。

### 2.1.1 集合通信的通信量

在 q 个设备为一组的BroadCast和Reduce中，时间为：

Tbroadcast=Treduce=Vlog(q)B ，其中V为通信量（每个设备上的数据），B为通信带宽。

megatron中的ring All-reduce所用时间：

Tring_allreduce=2(p−1)VpB

其他提到的操作：

Allgather：注意这里分母没有p，直观理解就是把某一设备上的V向外发送了p-1次。

Tall_gather=(p−1)VB .

Reduce-Scatter:

Treduce_scatter=(p−1)VpB .

### 2.1.2 等效函数(Isoefficiency function)

2D并行的论文里，用了这个函数作为和Megatron比较的指标。等效函数是一个并行策略的评估指标。

简单来说，就是：令 W 为单设备串行执行所需的时间。 p 为设备数量，要算出在什么条件下： E=Wp1Tp 为常数。

假设我们正在计算Transformer模型，则有 W∼O(h3) ，

1）Megatron的通信时间 TcommMegatron=2(p−1)pbshB≈O((p−1)ph2B) ，其中B为通信带宽，b为batch_size，s为seq_length，h为hidden_size，且设b正比例于hidden_size。则我们可算出 W∼h3∼p3 为其等效函数。即当 h∼p 时等效。

2）Optimus的通信时间 TcommOptimus=2qh2plog⁡q=h2plog⁡p 。则我们可算出 W∼h3∼(plog⁡p)3 时等效。

因为 plog⁡p<p ，所以作者认为基于2D模型并行的Optimus需要更少的设备就可以保证等效，因而Optimus要好于Megatron （擎天柱nb！）

### 2.1.3 内存开销

1) Megatron有内存瓶颈，因为每一层都要保留完整的激活值(activations)
2) SUMMA可以将激活值分布式地放到各个设备上，很适合解决内存问题。

## 2.2 张量并行(Megetron-LM)

在训练大模型的时候，通常一块GPU无法储存一个完整的模型。张量并行便是一种使用多块GPU存储模型的方法。与数据并行不同的是，张量并行是针对模型中的张量进行拆分，将其放置到不同的GPU上。比如说对于模型中某一个线性变换 ***Y=AX*** ，对于矩阵***A***有按列拆解和按行拆解两种方式：

![dc672154c5b45bb955640668272f5b3c.png](https://img-blog.csdnimg.cn/img_convert/dc672154c5b45bb955640668272f5b3c.png)

我们可以将矩阵***A~1~***和***A~2~***分别放置到两块不同的GPU上，让两块GPU分别计算两部分矩阵乘法，最后再在两张卡之间进行通信便能得到最终的结果。同理也可以将这种方法推广到更多的GPU上，以及其他能够拆分的算子上。

下图是 Megatron-LM [2] 在计算 MLP 的并行过程，它同时采用了这两种并行方式：

![d1bc036bef3b3384b1504120b8959501.jpeg](https://img-blog.csdnimg.cn/img_convert/d1bc036bef3b3384b1504120b8959501.jpeg)

整个MLP的输入***X***先会复制到两块GPU上，然后对于矩阵***A***采取上面提到的按列划分的方式，在两块GPU上分别计算出第一部分的输出***Y~1~***和 ***Y2*** 。接下来的 Dropout 部分的输入由于已经按列划分了，所以对于矩阵***B***则采取按行划分的方式，在两块GPU上分别计算出***Z~1~***和 ***Z2*** 。最后在两块GPU上的***Z~1~***和***Z2***做All-Reduce来得到最终的 ***Z*** 。

以上方法是对矩阵的一维进行拆分，事实上这种拆分方法还可以扩展到二维甚至更高的维度上。在Colossal-AI 中，他们实现了更高维度的张量并行：

> https://arxiv.org/abs/2104.05343 https://arxiv.org/abs/2105.14500 https://arxiv.org/abs/2105.14450

对于序列数据，尤洋团队还提出了Sequence Parallel来实现并行：

> https://arxiv.org/abs/2105.13120

* Megatron-LM: Efficient Large-Scale Language Model Training on GPU Clusters
* Mesh-Tensorflow:  Mesh-TensorFlow: Deep Learning for Supercomputers
* 夸父：Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training

## 2.3 流水线并行

和张量并行类似，流水线并行也是将模型分解放置到不同的GPU上，以解决单块GPU无法储存模型的问题。和张量并行不同的地方在于，流水线并行是按层将模型存储的不同的GPU上。比如以Transformer为例，流水线并行是将连续的若干层放置进一块GPU内，然后在前向传播的过程中便按照顺序依次计算hidden state。反向传播也类似。下图便是流水线并行的示例：

![1b5cd324b3c9bd8520a5dc5f0a4e1be1.png](https://img-blog.csdnimg.cn/img_convert/1b5cd324b3c9bd8520a5dc5f0a4e1be1.png)

但朴素的流水线并行实现会导致GPU使用率过低（因为每块GPU都要等待之前的GPU计算完毕才能开始计算），使流水线中充满气泡，如下图所示：

![cfe79e4c52458df236f89df486662f67.png](https://img-blog.csdnimg.cn/img_convert/cfe79e4c52458df236f89df486662f67.png)

有两种比较经典的减少气泡的流水线并行算法：GPipe [7] 和 PipeDream [8]

### 2.3.1 GPipe算法

GPipe 方法的核心思想便是输入的minibatch划分成更小的 micro-batch，让流水线依次处理多个 micro batch，达到填充流水线的目的，进而减少气泡。GPipe 方法的流水线如下所示：

![cd077d16022e5bf6878001bfbdd5c7dc.png](https://img-blog.csdnimg.cn/img_convert/cd077d16022e5bf6878001bfbdd5c7dc.png)

Gpipe和DAPPLE

Google最早提出的同步流水线，每一个minibatch计算完后进行同步后再计算下一个minibatch。

* GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism

### 2.3.2 PipeDream算法

PipeDream 解决流水线气泡问题的方法则不一样，它采取了类似异步梯度更新的策略，即计算出当前 GPU 上模型权重的梯度后就立刻更新，无需等待整个梯度回传完毕。相较于传统的梯度更新公式：

![62c54c6fdb079268321c8e57ef3d1428.png](https://img-blog.csdnimg.cn/img_convert/62c54c6fdb079268321c8e57ef3d1428.png)

PipeDream 的更新公式为：

![5882bf25b3bdca94890814fb6a3101d1.png](https://img-blog.csdnimg.cn/img_convert/5882bf25b3bdca94890814fb6a3101d1.png)

由于这种更新方式会导致模型每一层使用的参数更新步数不一样多，PipeDream 对上述方法也做出了一些改进，即模型每次前向传播时，按照更新次数最少的权重的更新次数来算，即公式变为：

![32361ce18217ca5626576d451a7fd7cd.png](https://img-blog.csdnimg.cn/img_convert/32361ce18217ca5626576d451a7fd7cd.png)

PipeDream 方法的流水线如下所示：

![1972a90803ac6974a5fd1ffb30bad9ee.png](https://img-blog.csdnimg.cn/img_convert/1972a90803ac6974a5fd1ffb30bad9ee.png)

参考：

* PipeDream: Fast and Efficient Pipeline Parallel DNN Training
* PipeDream-2BW: Memory-Efficient Pipeline-Parallel DNN Training
* HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism

### 2.3.3 DAPPLE算法

在具有多样化互连功能的复杂 GPU 平台上训练大模型(DAPPLE不仅限于LLM)是一项具有挑战性的任务。最近，流水线训练被提议作为提高设备利用率的有效方法。然而，仍然有几个棘手的问题需要解决：

1. 在保证收敛的同时提高计算效率，在不产生额外计算成本的情况下减少内存使用；
2. DAPPLE结合了大模型的数据并行和流水行并行，通过一种新颖的并行化策略规划器来解决分片问题，并探索数据并行和流水行并行最佳混合策略；
3. DAPPLE提出调度算法来减少设备内存使用，这与recompute正交并且不会以训练吞吐量为代价；
4. 实验表明，在同步训练场景下，DAPPLE Planner始终优于 PipeDreams Planner生成的策略，加速高达 3.23 倍，DAPPLE 运行时训练吞吐量加速优于 GPipe 1.6 倍，同时节省 12% 的内存消耗。

**同步流水线训练的效率问题**

流水线训练引入了相连计算设备之间的数据依赖。如GPipe[1]所描述， 提高计算设备利用率的一种常见方法是将mini-batch分成多个micro-batch。这些micro-batch以流水线方式安排在不同计算设备上同时执行。如文章GPipe所示，mini-batch被拆分的micro-batch越多，整体的流水线效率越高，但是却会引入两个问题：

1. 额外的内存消耗：GPipe的前向计算过程将所有的micro-batch连续计算， 然后计算出的activation用于反向计算。activation必须保存在所有微批次的内存中，直到它们相应的反向计算开始。因此，虽然更多的micro-batch意味着更高的效率，但内存限制限制了允许的micro-batch数量；
2. 冗余计算：GPipe等采用recompute来减少峰值内存消耗，即在前向计算丢弃一些activation，并在需要时在反向阶段重新计算它们，这里引入的冗余计算会带来额外的执行时间成本；

**流水线规划遇到的问题**

为了最大限度地提高流水线并行的资源利用率和训练吞吐量，找到一个好的策略来切分stage和将stage映射到计算设备是至关重要的。DAPPLE将stage划分和计算设备设备映射称为流水线规划。流水线会遇到以下问题：

1. 同步流水线能够保障收敛，而收敛是训练的前提。与异步流水线训练相比，同步流水线在所有micro-batch结束时需要一个额外的步骤来同步参数更新，比如allreduce， 这里会引入一定的通信开销，当前的流水线规划器没有考虑这种开销，因此无法准确地对端到端流水线训练时间进行建模；
2. 以前的方法没有考虑阶段数量的影响，在固定micro-batch数量和通信/计算开销比的情况下，阶段数越少，流水线效率越高；
3. 现有方法尚未对不均匀的模型切分有很好的研究，前面我们提到DeepSpeed有常见的三种模型切分方法，尽量保证计算设备负载均衡；

DAPPLE整体架构

![](https://pic1.zhimg.com/80/v2-ea10da11aac7a9690c158853750f6750_1440w.webp)

上图展示了DAPPLE的workflow， 包括DAPPLE Profiler、DAPPLE Planner、DAPPLE Runtime，基本流程如下：

1. DAPPLE Profiler用户的 DNN 模型，每一层的执行时间、activation大小和模型参数大小作为输入；
2. Profiler产生的结果作为输入， DAPPLE Planner在给定的全局批量大小上生成优化的（混合）并行化计划；
3. DAPPLE Runtime获取Planner的结果，并将原始模型图转换为流水线并行图。

DAPPLE Planner其中，planner旨在最小化一次训练迭代的端到端执行时间（考虑通信开销、global batch size等），如解决不均匀的模型stage切片。该模块负责Stage切分、设备分配，并生成最优并行化计划。特别是，设备分配是会考虑到硬件的拓扑信息；

DAPPLE还探索了将pipeline的单个stage映射到多个计算设备上。下图给出了一个示例，其中模型分为两个阶段，每个stage内部为DP组，通过高效的Nvlink来进行参数的更新，不同stage组成PP组， 通过以太网传递activation和grad：

![](https://pic4.zhimg.com/80/v2-eb2059d0d971b97ff8d395c890356243_1440w.webp)

DAPPLE Schedule

为了提高流水线训练效率，GPipe [1] 提出将全局批次拆分为多个micro-batch。然而，必须为所有micro-batch保留前向计算产生的激活函数，直到相应的反向计算开始，从而导致显存需求与micro-batch数成正比（如上图C所示，注意图中表示有问题红色为DAPPLE，蓝色为GPipe），这与我们尽量保证较大的micro-batch以提到较高的性能有冲突。而在DAPPLE 中，我们提出尽早地向后调度以减少内存消耗，即1F1B：

![](https://pic2.zhimg.com/80/v2-e979be00e4dfefc713bf71502d4ffb05_1440w.webp)

1F1B主要思想是尽早地反向计算，从而释放用于存储相应前向计算产生的激活的内存。如上图b:

1. 开始计算时，不是一次性喂入所有microbatch，以释放内存压力，同时保持高流水线效率；
2. 其次，严格地约束每个micro-batch的前向计算完成后进行反向计算，以保证反向计算能更早地进行。

上图C为GPipe 和 DAPPLE中的内存消耗如何随时间变化。开始时，DAPPLE 中的内存使用量随时间增加，直到喂入K个micro-batches（K<整个microbatch数，通常为stage数），随后开始反向计算，释放显存。由上图也可发现，这里与GPipe的bubble时间占比一致，但peak memory不随micro-batch数量增加，而是和stage数相关。

Planner和runtime的角色功能以及要解决的问题前面有描述， 具体的算法这里因为没有涉及到pipeline本身，就不详细讲了（后续在自动并行的总结中再做介绍）， 直观来说DAPPLE的架构设计比较全面，比较复杂。因为其复杂性，也约束在开源社区的支持，目前看起来DAPPLE本身架构在开源社区的支持比较少，但是1F1B的思路目前在DeepSpeed已是默认选择。

* DAPPLE: A Pipelined Data Parallel Approach for Training Large Models
* 双向化的GPipe：Chimera: efficiently training large-scale neural networks with bidirectional pipelines

### 2.3.3 interleaved 1F1B

Nvidia提出了一套interleaved 1F1B的方法(当然这篇文章更重要的是tensor并行的工作)，可以进一步地减少bubble耗时占比，其基本思路也是减少计算设备计算时地依赖， GPipe从数据层面切，而interleaved 1F1B则是从模型stage层面考虑，减少每次计算的粒度以获取更大的并行性：

![](https://pic2.zhimg.com/80/v2-bf0dc55c78b2257f831749dea5f4ee9d_1440w.webp)

如上图，将stage 切分变化后，Device1可以尽快完成stage1（L1）的计算以尽快进行其他micro-batch或者其他stage5的计算，详细时序图可由下图展示（其中浅色可看做stage5-stage8）：

综上，bubble时间占比从(n-1)/(n-1+m)优化为(n-1)/(n-1+km)，k为每个计算设备上的stage数。

* Narayanan, Deepak, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, et al. 《Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM》

### 2.3.4 TeraPipe算法

大语言模型进行训练时，通常需要使用大sequence length来保持住语言序列中的长依赖情况，然而大sequence length的内存开销比较大，就需要用更小的minibatch来训练，才能保证切分后的模型内存开销足够小，能够放的进设备中去。Gpipe并行，minibatch越小，pipeline stage中间的空闲bubble就越大，导致并行加速比降低。论文提出了一种在sequence维度进行更细粒度pipeline并行的计算方法，可以显著提高训练性能。

**Opportunity** ：Transformer是多层Layer堆叠而成，每层layer包含SelfAttention和FeedFroward操作。

![](https://pic2.zhimg.com/v2-b456e3fa1644d240cd81f4a3c3317649_r.jpg)

h_i是hidden state, 对应着input sequence中的每个position。SelfAttention的计算只依赖于t之前的hidden state，而FeedFroward只依赖于h_t自己。它们都不依赖于未来的hidden state，这样的结构使得把input sequence切开并行成为可能。也就是在Transformer结构中，当前layer处理当前token时，下一个layer处理上个token。如下图中(c)和(d)对比。

![](https://pic4.zhimg.com/v2-9284c2b0f947184179ff46af5ac00b9b_r.jpg)

切分input sequence (token dimension)可以和其他模型并行方式组合使用，如pipeline并行和拆分算子并行。在给定input sequence [x1, x2, …, xL]，如何找到合适切分点使得切分后[s1, s2, …, sM]，其中si包含[xl,…, sr]，使得端到端的训练效率最高。

**解决方法** ：选择合适的切分点很重要。若切分后的sequence太小，会使得设备利用率低；若太大，会使得bubble变大。同时，input sequence不能均分，h_t的计算依赖于之前的h1, …, h_t，处于模型后端的layer的computation load更大。下图是使用input sequence均分和运行时间均分的对比图。

![](https://pic3.zhimg.com/v2-5dc1d7e73081413c1961a2354919df92_r.jpg)

在给定pipeline切分stage数的前提下，文章提出了用动态规划的算法找到合适的input sequence切分点。该算法可扩展到同时切分batch维和input sequence。

**效果** ：训练GPT-3 175B模型时，利用384 GPU环境，相比于不切分input sequence的Gpipe，此工作的切分input sequence的方法会有5-6倍的性能提升。

**评论：** 论文中提到的这种逐字计算的方式，最初的驱动力是由于Gpipe这种方式内存开销大，不过，解决Gpipe并行方式内存开销大的问题，业界还有其他的方式，例如DeepSpeed、Dapple都有对Gpipe的改进方案，一样可以做到很高的并行并行加速比和低内存开销。笔者认为TeraPipe算是很好的一种技术路径探索，不过从训练的角度上看，实现上可能还是DeepSpeed、Dapple那种更加的高效。

> https://github.com/zhuohan123/terapipe
