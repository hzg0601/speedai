# 2 模型并行

## 2.1 朴素模型并行

## 2.2 张量并行(Megetron-LM)


* Megatron-LM: Efficient Large-Scale Language Model Training on GPU Clusters
* Mesh-Tensorflow:  Mesh-TensorFlow: Deep Learning for Supercomputers
* 夸父：Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training

## 2.3 流水线并行

### 2.3.1 GPipe算法

Gpipe和DAPPLE

Google最早提出的同步流水线，每一个minibatch计算完后进行同步后再计算下一个minibatch。

* GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
* 阿里的DAPPLE，目前业界最流行的流水线，Megatron的PipeDream-1F1B其实跟这个本质上是一个东西:  DAPPLE: A Pipelined Data Parallel Approach for Training Large Models
* 双向化的GPipe，个人看好的一种内存计算折中方案：Chimera: efficiently training large-scale neural networks with bidirectional pipelines

### 2.3.2 PipeDream算法

参考：

* PipeDream: Fast and Efficient Pipeline Parallel DNN Training
* PipeDream-2BW: Memory-Efficient Pipeline-Parallel DNN Training
* HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism
