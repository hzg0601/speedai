# 2 数据并行

## 2.1 朴素的数据并行(DDP)

数据并行是目前最为常见和基础的并行方式。这种并行方式的核心思想是对输入数据按 batch 维度进行划分，将数据分配给不同GPU进行计算。在数据并行里，每个GPU上存储的模型、优化器状态是完全相同的。当每块GPU上的前后向传播完成后，需要将每块GPU上计算出的模型梯度汇总求平均，以得到整个batch的模型梯度。

数据并行的核心之一是如何如何进行梯度汇总以更新参数，主流通信方式有scatter,gather,reduce,all_gather,all_reduce,ReduceScatter，其中参数服务器是一种ReduceScatter，而Ring Reduce分为Scatter Reduce和All Gather两个环节。

目前 PyTorch 已经支持了数据并行 [1]：

> https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html

## 2.2 ZeRO数据并行（ZeRO-DP）

ZeRO技术是微软的 DeepSpeed 团队解决数据并行的中存在的内存冗余问题所提出的解决方法。常驻在每块GPU上的数据可以分为三部分：模型参数，模型梯度和优化器参数。注意到由于每张 GPU 上都存储着完全相同的上述三部分参数，我们可以考虑每张卡上仅保留部分数据，其余的可以从其他 GPU 上获取。即假如有***N***张卡，我们可以让每张卡上只保存其中***1/N***的参数，需要的时候再从其他 GPU 上获取。ZeRO 技术便是分别考虑了上述三部分参数分开存储的情况，下图中的 ***Pos*** 、***Pos+g***和***Pos+g+p***就分别对应着将优化器参数分开存储、将优化器参数和模型梯度分开存储以及三部分参数都分开存储三种情况。论文里不仅分析了三种情况可以节省的内存情况，还分析出了前两种优化方法不会增加通信开销，第三种情况的通信开销只会增加50%。

ZeRO包含模型并行ZeRO-R和数据并行ZeRO-DP，这里我们只讨论数据并行ZeRO-DP。ZeRO-DP的出发点是优化model states，这里的model states包括：optimizer states, gradients and parameters，其中optimizer states前面已经说过，就是optimizer所需要的参数，对于Adam其optimizer states是parameters的2倍，而且使用混合精度训练时，optimizer states是fp32，这将成为显存占用的大头。

![](https://pic3.zhimg.com/v2-92492493246ce28fa45a7c947d5bf65a_r.jpg)

在混合精度训练中，训练的forward和backward采用的是fp16 weights，activations和gradients，但是weight update需要采用fp32，这就需要optimizer保存一份fp32 weights，而且optimizer states也要采用fp32。假定模型大小是 Ψ，而gradients和parameters均采用fp16，那么消耗的显存是2Ψ+2Ψ。而Adam需要fp32的parameters，momentum和variance（optimizer states），其消耗的显存是4Ψ+4Ψ+4Ψ。用K来表示optimizer states的multiplier，那么model states消耗的显存是（4+K)*Ψ，对于Adam来说K=12，那么model states消耗的显存是16Ψ。ZeRO-DP的优化策略就是分别对model states各个部分进行partitioning：

### **Optimizer State Partitioning**

如果DP的并行度为Nd（replicas数量），那么可以将optimizer state均分为Nd个partitions，这样第i个节点只需要更新optimizer state第i个partition。此时每个节点只需要存储和更新所有optimizer state的1Nd，而且也只更新parameter的1Nd。在每个training step的最后，只需要执行all-gather，每个节点就可以获得更新后的全部parameter。可以计算，optimizer State partitioning（Pos）消耗的显存就减少为4Ψ+K∗ΨNd。这个优化其实前面谷歌的工作也做了。

### **Gradient Partitioning**

既然每个节点只需要更新parameter的1Nd，那么其实每个节点也只需要对应参数的gradient。具体地，在backward过程的每个layer，一旦得到了gradient，每个节点就对自己所需那部分参数的gradient做reduce（等价于做一个reduce-scatter），得到summed gradients，由于其它部分的gradient并不需要了就可以释放了，从而减少了显存使用，这可以称为gradient partitioning（Pg）。此时显存的消耗降为2Ψ+(2+K)∗ΨNd。

### **Parameter Partitioning**

更进一步地，其实每个节点只需要存储要更新的那部分参数就好，在forward和backward过程中，需要全部的weight时再进行all-gather，然后再丢弃，这就是parameter partitioning（Pp），此时显存的消耗进一步减低为(4+K)∗ΨNd。但是采用parameter partitioning是有通信开销的，论文中实验说明使用后通信成本增大1.5倍。

基于ZeRO-DP，当Nd=1024时，1T Model（万亿参数）消耗的显存为15.6GB，模型可以放在一张32GB的V100卡上。

![](https://pic4.zhimg.com/v2-0d13ea800e8ba624f11957e283ce8017_r.jpg)

其实可以看到，谷歌的sharding weight update近似等价于采用Pos的ZeRO-DP，虽然两个工作的出发点不一样，但是殊途同归。在FSDP之前，Facebook已经实现了optimizer state+gradient sharding DP，这就是采用Pos+g的ZeRO-DP，或者叫ZeRO-DP-2，实现包含在[fairscale](https://github.com/facebookresearch/fairscale)库中，deepspeed团队实现了完整的ZeRO-DP：

> https://deepspeed.readthedocs.io/en/latest/zero3.html

## 2.3 FSDP

近期，Facebook发布了[FSDP（Fully Sharded Data Parallel）](https://engineering.fb.com/2021/07/15/open-source/fsdp/)，这个是对标微软在[DeepSpeed](https://github.com/microsoft/DeepSpeed)中提出的[ZeRO](https://arxiv.org/pdf/1910.02054.pdf)，FSDP可以看成PyTorch中的[DDP](https://pytorch.org/docs/stable/notes/ddp.html)优化版本，本身也是数据并行，但是和DDP不同的是，FSDP采用了parameter sharding，所谓的parameter sharding就是将模型参数也切分到各个GPUs上，而DDP每个GPU都要保存一份parameter，FSDP可以实现更好的训练效率（速度和显存使用）。这背后的优化逻辑可以从谷歌和微软的论文中找到。

### **Sharding weight update**

对于典型的数据并行实现（PyTorch的DDP和TF的[tf.distribute.MirroredStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy)）来说，每个replica（GPU）都拥有一份模型参数和一套optimizer，每个训练step，数据被均分到每个replica上，每个replica基于被分到的数据单独计算自己的local gradients，然后所有的replicas基于all-reduce操作来得到local gradients的summed gradients，这样每个replica其实都拿到了global gradients，最后基于global gradients更新模型参数（weight update）。这个过程如下图所示：

![](https://pic1.zhimg.com/v2-65f026e213763a1cac92f3b40ae32724_r.jpg)

其中all-reduce操作（ring all-reduce）包含两个操作：reduce-scatter和all-gather。在reduce-scatter阶段，gradients被均分成不同的blocks或shards，通过N-1轮交换数据，每个replica都得到一份reduced后的shards；在all-grather阶段，通过N-1轮数据交换，每个replica都将自己的那份reduced后的shards广播到其它的replicas，这样所有的replicas就能得到全部reduced后的gradients。不论有多少replicas，all-reduce的通信成本上是恒定的，这样就可以实现线性加速。

![](https://pic1.zhimg.com/v2-7fb98a5c3240afda9419e61d4a7fee6c_r.jpg)

每个replicas拿到reduced gradients后都在做重复的update weight，因为每个replicas都有模型参数的一个copy。如果模型（如NLP中的Transformer）比较大，参数量多，这个update weight在训练step中就会占据不可忽略的耗时；对于小模型的大规模分布式训练，一般每个device会采用较小的batch size以防止global batch size过大，此时update weight也会成为训练step中的重要耗时项。为了解决这个问题，谷歌在2020年提出了[sharding weight update](https://arxiv.org/pdf/2004.13336.pdf)，如下图所示，经过reduce-scatter后每个replica得到一个gradient shard，每个replica先更新自己的shard的weight，然后再进行all-gather，这样其实是和原始的all-reduce是等价的。但是经过这个调整，每个replica只是update weight shard，耗时就会降低了，相当于update weight也被各个replica给分担了。

![](https://pic1.zhimg.com/v2-33523460c929c8eaa3d6d5a2805cc228_r.jpg)

另外一点就是要考虑optimizer，因为optimizer往往包含额外的参数，比如SGD包含一套参数：gradient的EMA，而Adam包含两套参数：gradient的EMA和variance，这些参数可以统称为optimizer states，它们也是需要同步更新的。当模型参数较大时，optimizer states也会很大，比如Adam就是模型参数的2倍，如果也对optimizer states进行all-gather的话，通信成本就会比较大（原始的all-reduce并不需要）。optimizer states只参与weight update中，但是在下一个forward和backward中并不需要，不过optimizer states应该被包含在模型的checkpoints中，因为它们也是training state，比较好的方案是只有当需要时才对optimizer states进行all-gather，这就变成如下图所示：

![](https://pic3.zhimg.com/v2-64e6435702fd3867c7d0db766327cfbe_r.jpg)

这里optimizer的auxliary只在Loop body外面才进行all-gather以得到final auxliary。另外昨图和右图的区别是weight的all-gather的位置不同，左图weight的all-gather是在update后立即进行的，而右图是在需要的时候（forward和backward）才进行all-gather，看起来像是左边的方案更好一点，因为在最后得到final weight时右图还需要一次all-gather。但是右图方案有更大的优化空间，这是因为在forward和backward过程中往往不需要高精度的weight，比如TPU中可以采用bfloat16，虽然update weight需要float32。在右图方案中，可以采用低精度bfloat16来all-gather来得到所需要的全部weight，这样就大大降低了内存使用和通信成本。另外weight和auxliary weight的生存周期也减少了。特别是optimizer的auxliary weight，在training loop中其实只需要shard，这样就节省一部分内存空间，可以用来存储forward和backward中activations和gradients。假定模型参数大小是W，而auxliary weight大小是V，共有N个shards，orward和backward中activations和gradients的峰值大小是P，那么训练的峰值大小就从W+V+P降低为max(W+V/N+P，W+V)，这带来的一个好处是Adam将和SGD一样高效（Adam比SGD要多一份auxliary weight）。

![](https://pic1.zhimg.com/v2-76b6a9bd97dc29488907ac5a7c7fc75c_r.jpg)

可以看到谷歌提出的sharding weight update不仅可以加速训练，而且也会节省显存，这里只是简单介绍了论文最核心的优化逻辑，论文中还有关于graph和shard具体实现细节讨论。论文中基于ResNet-50，Transformer和NCF三个模型做实验，实验配置如下：

![](https://pic1.zhimg.com/v2-97dd082a1413cf61a6eecec4700fd8a8_r.jpg)

从实验结果来看，无论是CV还是NLP模型在训练耗时和显存使用上均有提升，特别是对大规模训练的场景（replica batch size小）和模型较大的场景（Transformer模型）：

![](https://pic2.zhimg.com/v2-02dd098c2901049446e4423c5f83678d_r.jpg)



facebook的实现包含在[fairscale](https://github.com/facebookresearch/fairscale)库中, pytorch官方也有相应实现。

> https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html

> https://github.com/facebookresearch/fairscale

## 2.3 序列并行

Sequence Parallelism: Making 4D Parallelism Possible

自注意力机制是Transformer中的一个关键部件，但其占用显存大小和序列长度呈平方关系，导致我们实际并不能用很长的序列（如BERT是固定为512）。在这篇工作里，我们提出了 **序列并行** (Sequence Parallelism)，将序列切分成一个个小块，放置在每一个计算设备上。计算注意力需要将Query和全局的Key交互， **受启发于Ring-Allreduce算法** ，我们以类似的方式实现注意力计算，并称为 **Ring Self Attention** 。该并行方式能够与现有的数据并行，模型并行，流水线并行一起使用，实现4D并行。

### **Sequence Parallelism**

![](https://ask.qcloudimg.com/http-save/yehe-4941972/041eb989f040769ea2e6136a1181737d.png?imageView2/2/w/1200)

左图是流水线并行，右图是模型并行，这两种并行方式已经非常流行了。

而这里提到的序列并行，则是针对 Transformer 训练更长序列提出的，整体结构图如下：

![](https://ask.qcloudimg.com/http-save/yehe-4941972/ee31989385a1d32ab50a9753e7c86725.png?imageView2/2/w/1200)

主要特点是，我们将整一个序列进行切分，分成多个子序列，放置在不同的device上。**每一个device上有完整的模型权重，和不同的子序列。**

### **MLP部分的序列并行**

这里我们的MLP特指Transformer的FFN模块，即输入经过两个全连接层，且使用 Adam 优化器:

```javascript
// 假设输入是 (B, L, H)
dense1 = nn.Linear(H, 4H)
dense2 = nn.Linear(4H, H)
```

如果是模型并行，那么第一个全连接层的权重将在第1维进行切分，即每个设备上的权重大小为$(H,\frac{4H}{N}$)，输出结果为$(B,L,\frac{4H}{N})$。而第二个全连接层的权重将在第0维进行切分，即每个设备上的权重大小为 $(\frac{4H}{N},H)$，然后进行运算，整个过程所需的显存为：

$\frac{4H^2}{N}*2$(两个全连接层参数大小)
$H*\frac{4H^2}{N}*2$（两个全连接层参数对应梯度)

$H*\frac{4H^2}{N}*2$(两个全连接层参数对应Adam的m，n两个变量)

$(\frac{4BLH}{N}$)第一层全连接输出结果激活值)

$BLH$(第二层全连接输出结果激活值)

加起来就是$\frac{32H^2}{N}+{4BLH}{N}+BLH$.

接下来我们看下序列并行的情况，它在序列维度上切分，那么输入变为$(B, L/N , H)$，第一层全连接层权重还是$(H, 4H)$，输出为$(B, L/N , 4H)$。此时第二层全连接层权重是$(4H, H)$，输出为$(B,L/N , H)$，类似地我们计算出对应的显存占用:

$4H*H*4+\frac{4BLH}{N}+\frac{BLH}{N}$

![](https://ask.qcloudimg.com/http-save/yehe-4941972/83072ea5c4a6f4a251850eecf44cde2f.png?imageView2/2/w/1200)

`BL > 32H` 时,序列并行是比模型并行省显存的.

我们看通信开销方面，因为模型并行对权重进行切分，所以前向过程和后向过程都需要做一次all reduce操作，而序列并行不需要。

### **自注意力机制的序列并行**

**补充材料：Ring Allreduce**

在正式介绍自注意力序列并行之前，我们先简单介绍 Ring Allreduce算法。

> 这里笔者推荐一篇博客：ahttps://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/

该算法是为了解决多个GPU通信效率低下的问题而产生的，它将多个GPU连成一个环

![](https://ask.qcloudimg.com/http-save/yehe-4941972/8ed419f6c2b0ee0c065d656112c66844.png?imageView2/2/w/1200)

我们对每张卡上的数据进行分快，初始状态为：

![](https://ask.qcloudimg.com/http-save/yehe-4941972/05832e98dc1e744b909d61d1561987e3.png?imageView2/2/w/1200)

第一步做 scatter-reduce，每张卡在每次iter里同时send和recieve一个数据块：第一个iter：

![](https://ask.qcloudimg.com/http-save/yehe-4941972/60d1dafc5e184df12a6f562b6f822515.png?imageView2/2/w/1200)

第二个iter：

![](https://ask.qcloudimg.com/http-save/yehe-4941972/93e9e3ab2859ed21ce917965f162cac0.png?imageView2/2/w/1200)

这样最终结果是，**每张卡都有一个完整的数据块总和**

![](https://ask.qcloudimg.com/http-save/yehe-4941972/ee08c164b144b5faa115ca18ec0f0c1a.png?imageView2/2/w/1200)

第二步做 all-gather，每张卡在每次iter都send和recieve一个求和过的完整数据块。

第一个iter：

![](https://ask.qcloudimg.com/http-save/yehe-4941972/c81f0a0027297835c942bcaf06349f1f.png?imageView2/2/w/1200)

第二个iter：

![](https://ask.qcloudimg.com/http-save/yehe-4941972/0c1d9ea1a87ef58031120036a83e7876.png?imageView2/2/w/1200)

最后状态就是每张卡都有所有卡的数据总和：

![](https://ask.qcloudimg.com/http-save/yehe-4941972/aa0fd9288450636e0f6d0a7a49c04791.png?imageView2/2/w/1200)

那自注意力机制的序列并行也和ring all-reduce有着异曲同工之处，每张卡都只有子序列，而Q,K,V的计算有需要和所有序列进行交互。那么对应做法就是在每个iter的时候，传输各卡的一个子序列数据。

![](https://ask.qcloudimg.com/http-save/yehe-4941972/3e75d6119a255f4b6c727b001466d9a6.png?imageView2/2/w/1200)

我们以计算Q, K为例，在第一个iter中：

* Device1接收了Device4上的Key，计算了Device1，Device4的Q1K
* Device2接收了Device1上的Key，计算了Device2，Device1的Q2K
* Device3接收了Device2上的Key，计算了Device3，Device2的Q3K
* Device4接收了Device3上的Key，计算了Device4，Device3的Q4K

在加上后续2个iter，那么所有device都有完整的QK结果了。接下来计算Attention Scores也是类似的逻辑，每个卡都传输各自的value，得到最终的输出：

![](https://ask.qcloudimg.com/http-save/yehe-4941972/8563a436d5345523612509caabb04f5d.png?imageView2/2/w/1200)

与前面类似，我们比较在模型并行下，计算自注意力所需的显存大小（这里就不再推导了）：

![](https://ask.qcloudimg.com/http-save/yehe-4941972/27ba62446918b3fb3c2599443127929b.png?imageView2/2/w/1200)

通信开销方面，在模型并行下，前向后向各需要一次all-reduce；在序列并行下，前向需要两次 all-reduce（就是前面我们推导的Key，value传递的过程），后向需要四次 all-reduce。虽然在计算自注意力机制是多了一些all-reduce操作，但是在之前的MLP部分，序列并行比模型并行少了2次all-reduce，也算权衡了一些。

> 这里后向需要4次all-reduce笔者不是很理解，笔者觉得是反向计算 V的梯度，QK总梯度，Q的梯度，K的梯度所用到。笔者是系统小白，还望有人能指正解惑。

### **实验结果**

![](https://ask.qcloudimg.com/http-save/yehe-4941972/84baa5fa04d23dedf2ff18e344ae4ef1.png?imageView2/2/w/1200)

![](https://ask.qcloudimg.com/http-save/yehe-4941972/18daeb9d0e2a417842d21c38afe03e43.png?imageView2/2/w/1200)

代码已经被集成在 ColossalAI 里，只依靠 PyTorch 自带的通信API实现。通过调用PyTorch的通信API P2POp 来实现 send 和 recieve，添加至一个列表中。能够将一个tensor发送到下一个rank，并从上一个rank接收一个tensor。

代码地址：`https://github.com/hpcaitech/ColossalAI`
